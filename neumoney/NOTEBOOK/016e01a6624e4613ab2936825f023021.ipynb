{"cells":[{"metadata":{},"outputs":[],"source":"#### About this Notebook\nExport repayment.i2c_bank_card_transfer table as parquet file into S3 location. Table is ingested into Redshift as stage.ds_postgres_repayment_import_i2c_bank_card_transfer","cell_type":"markdown","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"!pip install awswrangler","cell_type":"code","execution_count":1},{"metadata":{"trusted":true},"outputs":[],"source":"import pandas as pd\nfrom sprinkleSdk import SprinkleSdk as sp\nimport boto3\nimport awswrangler as wr\nfrom datetime import datetime\nimport logging\nimport json\nimport redshift_connector","cell_type":"code","execution_count":2},{"metadata":{"trusted":true},"outputs":[],"source":"def getBoto3Session(aws_access_key, aws_secret_access_key, region):\n    boto3session = None\n    try:\n        boto3session = boto3.Session(aws_access_key_id=aws_access_key,\n                                aws_secret_access_key=aws_secret_access_key,\n                                region_name=region)\n    except Exception as e:\n        print(f\"getBoto3Session: {str(e)}\")\n    return boto3session\n\ndef getEnvConfig(env=None):\n    # Specify the AWS user's access key/secret key here.\n    AWS_ACCESS_KEY_ID = \"AKIAVMM664F75NS4NJOL\"\n    AWS_SECRET_ACCESS_KEY = \"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\"\n    AWS_REGION_NAME = 'us-east-1'\n    try:\n\n        botoSession = getBoto3Session(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME)\n        if not botoSession:\n            raise Exception(\"Failed to obtain boto session\")\n\n        if not env or env == 'DEV':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": \"neumoney-dev-redshift-cluster.crwu8dyj5u4p.us-east-1.redshift.amazonaws.com\",\n                \"REDSHIFT_PORT\": 5439,\n                \"REDSHIFT_DATABASE\": \"neumoney-dev\",\n                \"REDSHIFT_USER\": \"neumoney\",\n                \"REDSHIFT_PWD\": \"p+6JVumb>g)QkiV\"\n            }\n        if env == 'PROD':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": wr.secretsmanager.get_secret(\"REDSHIFT_HOST\", botoSession),\n                \"REDSHIFT_PORT\": wr.secretsmanager.get_secret(\"REDSHIFT_PORT\", botoSession),\n                \"REDSHIFT_DATABASE\": wr.secretsmanager.get_secret(\"REDSHIFT_DATABASE\", botoSession),\n                \"REDSHIFT_USER\": wr.secretsmanager.get_secret(\"REDSHIFT_USER\", botoSession),\n                \"REDSHIFT_PWD\": wr.secretsmanager.get_secret(\"REDSHIFT_PWD\", botoSession)\n            }\n\n    except Exception as e:\n        print(f\"getConfig: {str(e)}\")\n        return None\n    return config","cell_type":"code","execution_count":3},{"metadata":{"trusted":true},"outputs":[],"source":"class AwsS3():\n    def __init__(self, config):\n        self.AWS_ACCESS_KEY_ID = config.get(\"AWS_ACCESS_KEY_ID\")\n        self.AWS_SECRET_ACCESS_KEY = config.get(\"AWS_SECRET_ACCESS_KEY\")\n        self.region_name = config.get(\"AWS_REGION_NAME\")\n        self.bucket_name = config.get(\"BUCKET_NAME\")\n        self.boto3session = None\n        self.s3client = None\n\n    def initialize(self):\n        self.getBoto3Session()\n        self.getBoto3S3Client()\n        return True\n\n    def getS3Url(self, file_prefix):\n        url = f\"s3://{self.bucket_name}/{file_prefix}\"\n        return url\n\n    def getBoto3Session(self):\n        try:\n            session = boto3.Session(aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                                    aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n                                    region_name=self.region_name)\n            self.boto3session = session\n        except Exception as e:\n            print(f\"getBoto3Session: {str(e)}\")\n            return False\n        return True\n\n    def getBoto3S3Client(self):\n        try:\n            s3_client = boto3.client('s3', aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                                     aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY, region_name=self.region_name)\n            self.s3client = s3_client\n        except Exception as e:\n            print(f\"getBoto3S3Client: {str(e)}\")\n            return False\n        return True\n\n    def existS3Key(self, prefix):\n        found = False\n        try:\n            result = self.s3client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)\n            # result = self.s3client.list_objects_v2(Bucket='neumoney-stagging-thp-log', Prefix='thp-logs/22-02-2023/credit-card/uat/')\n            if 'Contents' in result.keys():\n                found = True\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return found\n\n    def existS3Key2(self, prefix,bucket_name):\n        found = False\n        try:\n            result = self.s3client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n            if 'Contents' in result.keys():\n                found = True\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return found\n\n\n    def json_file_prefix(self, df, env, db_name, table_name,json_col_name,merge_keys_json,bucket_name):\n        json_list = []\n        for index, row in df.iterrows():\n            row_json = dict()\n            file_prefix_json = '{}/{}/{}/'.format(env, db_name, table_name\n                                                           # commenting the following line since the year, date folder structure is not needed.\n                                                           #,str(row['modified_timestamp'])[0:4],\n                                                           #str(row['modified_timestamp'])[5:7],\n                                                           #str(row['modified_timestamp'])[0:10]\n                                                          )\n            file_prefix_json_full = '{}/{}/{}/{}_{}'.format(env, db_name, table_name,\n                                                                     #str(row['modified_timestamp'])[0:4],\n                                                                     #str(row['modified_timestamp'])[5:7],\n                                                                     #str(row['modified_timestamp'])[0:10],\n                                                                     table_name, \n                                                                     row[merge_keys_JSON[0]])\n            # file_prefix_json = 'dev/credit_limit_dev/scienaptic_details/2023/03/2023-03-02/'\n            # file_prefix_json_full = 'dev/credit_limit_dev/scienaptic_details/2023/03/2023-03-02/scienaptic_details_30dcaaa2-b8c6-11ed-b4bc-a35090bde0c0_T_2023_03_02_06_49_28_318431.json'\n            \n            \n            Path_list = self.s3client.list_objects_v2(Bucket=bucket_name, Prefix=file_prefix_json)            \n            \n            if 'Contents' in Path_list.keys():\n                found = True\n                print(\"file_prefix_json_full------\",file_prefix_json_full)\n            else:\n                found = False\n            if found:\n                particular_path = [item['Key'] for item in Path_list['Contents'] if\n                                   item['Key'].startswith(file_prefix_json_full)]\n                result = sorted(particular_path, reverse=True)\n                \n#                 print(\"result----\",result)\n                if not result:\n                    result = str()\n                else:\n                    result = str(result[0])\n                    print(result)\n\n                json_data = s3obj.readJSON(result,bucket_name)\n                if json_data['status'] == 'ok':\n                    stud_obj = json.loads(json_data['data'])\n                    row_json[json_col_name] = str(stud_obj)\n                    for ln in merge_keys_json:\n                        row_json[ln] = row[ln]\n\n                else:\n                    row_json[json_col_name] = str()\n                    for ln in merge_keys_json:\n                        row_json[ln] = row[ln]\n            else:\n                row_json[json_col_name] = str()\n                for ln in merge_keys_json:\n                    row_json[ln] = row[ln]\n            json_list.append(row_json)\n        return json_list\n\n\n    def fetchS3Key(self, prefix):\n        try:\n            Path_list = self.s3client.list_objects_v2(Bucket='neumoney-stagging-thp-logs', Prefix='thp-logs/22-02-2023/credit-card/uat/')\n            particular_path = [item['Key'] for item in Path_list['Contents'] if item['Key'].startswith(\n                'thp-logs/22-02-2023/credit-card/uat/credit_card_0e9884e2-b1f1-11ed-bcde-ebc0e70de44d')]\n            result = sorted(particular_path,reverse=True)\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return result[0]\n\n    def writeParquet(self, df, file_prefix, partition_cols, parquet_definition, glue_database, glue_table):\n        \"\"\"\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html\n        file_prefix: the prefix for the table. E.g: \"s3://neu-data-analytics/application_data/application_data_identity/user_data_v0\"\n        partition_cols: List of columns by which the parquet is to be partitioned\n        \"\"\"\n        resp = {}\n        try:\n            if df.empty:\n                raise Exception(\"Dataframe empty. Nothing to write to S3.\")\n\n            partitioned = True if partition_cols else False\n            s3url = self.getS3Url(file_prefix)\n            file_exists = self.existS3Key(file_prefix)\n\n            if file_exists:\n                # logging.info(f\"URL:{s3url} exists. Overwriting partitions...\")\n                print(logging.info(f\"URL:{s3url} exists. Overwriting partitions...\"))\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        mode='overwrite_partitions',\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition\n                                        )\n            else:\n                logging.info(f\"URL:{s3url} does not exist. Creating...\")\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition)\n\n            if not resp:\n                raise Exception(f'Failed to write parquet at: {s3url}')\n\n        except Exception as e:\n            print(f\"writeParquet: {str(e)}\")\n            return resp\n        return resp\n\n    def readParquet(self, file_prefix, partition_predicate):\n        \"\"\"\n        Reads entire parquet dataset if partition_predicate is None, If supplied, reads one partition. Returns a pandas dataframe.\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.read_parquet.html\n        ::file_prefix: Prefix for the table. E.g: \"s3://neu-data-analytics/persona_data/inquiry_data\"\n        ::partition_predicate: partition string: E.g: \"<column_name1>=<value>/<column_name2>=<value>/....\" format. Depends on the number of levels of partitioning\n        \"\"\"\n        df = pd.DataFrame()\n        urls = []\n        try:\n            # Read each partition from the supplied list.\n            s3url = self.getS3Url(file_prefix)\n\n            # If there are partitions specified, read only those specific partitions. Else read the entire parquet file\n            if partition_predicate:\n                url = f\"{s3url}/{partition_predicate}\"\n            else:\n                url = s3url\n\n            print(url)\n            df = wr.s3.read_parquet(path=url, dataset=True, boto3_session=self.boto3session, ignore_empty=True,\n                                    ignore_index=True, path_suffix=[\".snappy.parquet\"])\n            # print(\"DF type is this \",type(df),)\n        except Exception as e:\n            print(f\"readParquet: {str(e)}\")\n        return df\n\n    def readJSON(self, file_prefix,bucket_name):\n        \"\"\"\n        Reads a JSON file with the specified file_prefix from the S3 bucket.\n        Returns a dictionary of the format {\"data\": {..json data read from s3 file}, \"status\": \"ok|FileNotFound|error|...\"}\n        \"\"\"\n        resp = {\"data\": None, \"status\": None}\n        try:\n            if self.existS3Key2(file_prefix,bucket_name):\n                result = self.s3client.get_object(Bucket=bucket_name, Key=file_prefix)\n                if not result:\n                    raise Exception(\"ReadError\")\n                jsonbody = result.get(\"Body\").read().decode()\n                if not jsonbody:\n                    raise Exception(\"EmptyJson\")\n                resp = {\"data\": jsonbody, \"status\": \"ok\"}\n            else:\n                raise Exception(\"FileNotFound\")\n        except Exception as e:\n            resp = {\"status\": str(e), \"data\": None}\n            print(f\"readJSON: {str(e)}\")\n        return resp\n","cell_type":"code","execution_count":4},{"metadata":{"trusted":true},"outputs":[],"source":"def getUniquePartitionValues(df):\n    return df[[\"part_created_month\",\"part_created_date\"]].drop_duplicates().to_dict('records')","cell_type":"code","execution_count":5},{"metadata":{"trusted":true},"outputs":[],"source":"def fetchExistingPartitions(parts_list, file_prefix, df_schema):\n    \"\"\"\n    Given a list of partition predicates, read all those partitions and concatenate them into a single pandas dataframe\n    Each item in parts_list will be of the type: {\"part_created_month\": \"202210\", \"part_created_date\": \"20221005\"}\n    \"\"\"\n    part_dfs = []\n    existing_data_df = pd.DataFrame()\n    try:\n        for partition_predicate in parts_list:\n            df_part = s3obj.readParquet(file_prefix, f\"part_created_month={partition_predicate.get('part_created_month')}/part_created_date={partition_predicate.get('part_created_date')}\")\n            \n            if not df_part.empty:\n                df_part[\"part_created_month\"] = partition_predicate.get('part_created_month')\n                df_part[\"part_created_date\"] = partition_predicate.get('part_created_date')\n                part_dfs.append(df_part)\n            \n        if part_dfs:\n            existing_data_df = pd.concat(part_dfs)[df_schema] # Return only the columns which will be present in the incoming delta dataset. Strip all derived columns existing in parquet \n    except Exception as e:\n        print(f\"fetchExistingPartitions: {str(e)}\")\n    return existing_data_df","cell_type":"code","execution_count":6},{"metadata":{"trusted":true},"outputs":[],"source":"def mergeDeltaWithExistingParquet(df, file_prefix, partition_cols, merge_keys, df_schema, s3obj):\n    \"\"\"\n    ::df: dataframe having delta records for the batch\n    ::file_prefix: The parquet file name. E.g: persona_inquiry_data\n    ::partition_cols: Columns used as partition keys. Must be a list, column names in the order of partition hierarchy. E.g: [\"part_appcreated_month\", \"part_appcreated_date\"]\n    ::merge_keys: Unique identifying columns that can be used to join delta dataframe with existing parquet data, inorder to identify update candidates\n    ::final_schema: Final list of columns to be stored into parquet, in the exact order.\n    \"\"\"\n    if df.empty:\n        print(\"mergeDeltaWithExistingParquet: Delta records dataset is empty. Nothing to merge.\")\n        return df\n    \n    df_final = pd.DataFrame()\n    records_written = 0\n    try:\n        parquet_exists = s3obj.existS3Key(file_prefix)\n        \n        if not parquet_exists:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Does not exist.\")\n            df_final = df\n        else:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Exists. Preparing delta\")\n            \n            delta_partitions_list = getUniquePartitionValues(df)\n            existing_data_df = fetchExistingPartitions(delta_partitions_list, file_prefix, df_schema)\n    \n            # Left join existing records(in S3) with the incoming delta records. Use the columns that uniquely identifies a record\n            if not existing_data_df.empty:\n                # get only the merge columns from the delta set - just to do a left join with existing data\n                df_delta_filter = df[merge_keys].copy(deep=True).reset_index(drop=True)\n                df_delta_filter[\"new_delta\"] = 1\n                \n                # Join with the incoming delta set and get the new_delta column =1, if the record exists in the incoming delta set\n                existing_marked_df = existing_data_df.merge(df_delta_filter, how=\"left\", on=merge_keys).copy(deep=True)\n    \n                # Filter out records from existing S3 records, which found a match in the incoming delta set.\n                df_existing_portion = existing_marked_df.query(\"~(new_delta==1)\").copy(deep=True)\n                df_existing_portion[\"new_delta\"] = 0\n\n                df[\"new_delta\"] = 1 # Just to match the list of columns\n\n                # Now concatenate the incoming delta set (full), with those existing rows from S3 which does not exist in the new delta set.\n                # This makes the full record set for the target.\n                df_final = pd.concat([df_existing_portion, df])[df_schema]\n            else:\n                df_final = df\n\n    except Exception as e:\n        print(f\"mergeDeltaWithExistingParquet: {str(e)}\")\n    return df_final","cell_type":"code","execution_count":7},{"metadata":{"trusted":true},"outputs":[],"source":"# Write the final dataframe to parquet\ndef write2Parquet(df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table):\n    res = {}\n    try:\n        # Write the prepared dataset back into S3. \"overwrite_partitions\" mode is being used while writing into an existing parquet file.\n        # This will ensure that, if there is a record that needs to go into an existing S3 partition, then that entire partition is overwritten.\n        # The step above makes sure that the dataset being overwritten contains all records that must be present in the target partition.\n        if not df.empty:\n            df_final = df[parquet_schema].copy() # Selecting columns in the correct order\n            print(\"write2Parquet: Writing parquet...\")\n            resp = s3obj.writeParquet(df_final, file_prefix, partition_cols, parquet_definition, glue_database, glue_table)\n            if not resp: \n                raise Exception(f\"Failed to write into [{file_prefix}] parquet.\")    \n            res = {\"status\":True, \"records_written\": df.shape[0]}\n        else:\n            res = {\"status\":True, \"records_written\": 0}\n            print(\"write2Parquet: Final delta record set is empty. Nothing to write to S3.\")\n    except Exception as e:\n        print(f\"write2Parquet: {str(e)}\")\n        res = {\"status\":False, \"records_written\": 0}\n    return res\n    \ndef map_parquet_types_to_pandas(merge_keys, parquet_definition):\n    type_map = None\n    try:\n        type_map = {}\n        for col in merge_keys:\n            parquet_type = parquet_definition.get(col)\n            pandas_type = \"string\"\n            if parquet_type in ['bigint', 'integer']:\n                pandas_type = \"Int64\"\n            if parquet_type in ['double']:\n                pandas_type = \"float64\"\n            if parquet_type in ['date']:\n                pandas_type = \"datetime64\"                \n            # Add more if needed\n            \n            type_map[col] = pandas_type\n    except Exception as e:\n        print(f\"map_parquet_types_to_pandas: {str(e)}\")\n    return type_map  \n    \n\ndef updateBatchControl(df, timestamp_column_name, batch_code, rdb_conn):\n    \"\"\"\n    df: Dataframe containing all the delta records that were written into parquet\n    timestamp_column_name: Column containing the timestamp value (preferably modified date time) in epoch format\n    batch_code: The batch code of the table archival script (PK from batch_control table)\n    rdb_conn: Redshift connection object\n    \"\"\"\n    if df.empty:\n        return True\n\n    try:\n        # Calculate the max(timestamp) from the given timestamp column in the dataframe.\n        max_timestamp = res[timestamp_column_name].max()\n        # Convert epoch number into formatted string representation\n        next_start_time = datetime.fromtimestamp(max_timestamp / 1000.0).strftime('%Y-%m-%d %H:%M:%S.%f')\n        # Prepare update SQL for batch control\n        updsql = \"update stage.batch_control set LAST_TIMESTAMP=%s where BATCH_CODE=%s\"\n        updparams = [next_start_time, batch_code]\n        updresult = rdb_conn.execute(updsql, updparams)\n\n        if updresult is None or updresult != 1:\n            raise Exception(\"Batch_control update failed\")\n    except Exception as e:\n        print(f\"updateBatchControl: {str(e)}\")\n        return False\n    return True","cell_type":"code","execution_count":8},{"metadata":{},"outputs":[],"source":"class RedshiftDb(object):\n    def __init__(self, config):\n        self.env = config.get(\"env\")\n        self.logger = logging.getLogger(\"RedshiftDb\")\n        self.dbConn = self._get_db_connection(config)\n\n    def _get_db_connection(self, config):\n        dbConn = None\n        try:\n            dbConn = redshift_connector.connect(host=config.get(\"REDSHIFT_HOST\"),\n                                                port=config.get(\"REDSHIFT_PORT\"),\n                                                database=config.get(\"REDSHIFT_DATABASE\"),\n                                                user=config.get(\"REDSHIFT_USER\"),\n                                                password=config.get(\"REDSHIFT_PWD\")\n                                                )\n            if not dbConn:\n                raise Exception(\"Could not connect\")\n        except Exception as e:\n            self.logger.error(f\"connect: {str(e)}\")\n        return dbConn\n\n\n    def fetchAll(self, sql, params=None):\n        \"\"\"\n        Takes a SQL string optional list of parameters as input\n        Returns pandas dataframe if the query was successful and has records.\n        On error or when result is empty, returns a None\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            cursor.execute(sql, params)\n            result = cursor.fetch_dataframe()\n\n        except Exception as e:\n            self.logger.error(f\"fetchall: {str(e)}\")\n        return result\n\n\n    def execute(self, sql, params=None):\n        \"\"\"\n        Executes one sql with an optional list of parameters\n        Returns number of records affected, when the SQL is successful. None in case of errors\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            res = cursor.execute(sql, params)\n            self.dbConn.commit()\n            result = res.rowcount\n\n        except Exception as e:\n            self.logger.error(f\"execute: {str(e)}\")\n            if self.dbConn:\n                self.dbConn.rollback()\n        return result","cell_type":"code","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"#Connection\nprint(f\"Starting Batch Job: Incremental dump of repayment.i2c_bank_card_transfer table data into Parquet\")\nconfig = getEnvConfig(env=\"DEV\") # Change env=\"PROD\" while deploying to production\n\nif not config:\n    print(f\"ERROR: Could not get credential configurations.\")","cell_type":"code","execution_count":9},{"metadata":{"trusted":true},"outputs":[],"source":"s3obj = AwsS3(config)\ns3obj.initialize()\n\n#rdb = RedshiftDb(config)  # <<<<<---- Uncomment once Sprinkle moves to neumoney n/w\nbatch_code = \"REPAYMENT_I2C_BANK_CARD_TXFR\"\ntimestamp_column = \"modified_date\"\n\nglue_database = \"archive_redshift_db\"\nglue_table = \"repayment_i2c_bank_card_transfer\"\n\nfile_prefix = \"application_data/repayment/i2c_bank_card_transfer_v0\"\npartition_cols = [\"part_created_month\", \"part_created_date\"]","cell_type":"code","execution_count":10},{"metadata":{"trusted":true},"outputs":[],"source":"# Read the delta records from source, using the explore object. (Explore name: i2c_bank_card_transfer_data)\nexplore_id = 'dd910342515843599a657d2ef3ee5c45' #i2c_bank_card_transfer_data \ndf = sp.read_explore(explore_id)\n\nsql = (\"select  i2c.id, i2c.account_sr_no, \"\n    \"  i2c.bank_account_detail_uuid, \"\n    \"  i2c.credit_card_uuid, i2c.card_reference_id, \"\n    \"  i2c.transfer_frequency, \"\n    \"  i2c.transfer_date, \"\n    \"  i2c.transfer_end_date, \"\n    \"  i2c.transfer_count, i2c.transfer_id, i2c.transfer_result_type, i2c.transfer_status, \"\n    \"  i2c.response_code, i2c.response_desc, i2c.trans_id, \"\n    \"  i2c.created_date, i2c.modified_date, i2c.recurrence_id,\"\n    \"  TIMESTAMP 'epoch' + i2c.created_date/1000 *INTERVAL '1 second'  as created_timestamp, \"\n    \"  TIMESTAMP 'epoch' + i2c.modified_date/1000 *INTERVAL '1 second'  as modified_timestamp \"\n    \" from stage.ds_postgres_repayment_import_i2c_bank_card_transfer i2c \"\n    \" left join stage.batch_control bc  \"\n    \" on date_add('ms',i2c.modified_date,'1970-01-01')  >= bc.last_timestamp \"\n    \" where bc.batch_code = %s; \"\n    )\nsql_params = [batch_code]\n# Uncomment once Sprinkle has moved to neumoney network\n# Fetch data from stage table in redshift\n# df = rdb.fetchAll(sql, sql_params)","cell_type":"code","execution_count":22},{"metadata":{"trusted":true},"outputs":[],"source":"# Convert the  created datetime into YYYYMMDD format for partitioning key\ndf[\"part_created_month\"] = df[\"created_timestamp\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m\"))  \ndf[\"part_created_date\"] = df[\"created_timestamp\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m%d\"))  \nprint(f\"Fetched {df.shape[0]} records to be added into [{file_prefix}] S3 file\")","cell_type":"code","execution_count":23},{"metadata":{"trusted":true},"outputs":[],"source":"# Columns used to identify unique records from the data set\nmerge_keys = ['id', 'bank_account_detail_uuid', 'part_created_month','part_created_date']\nmerge_keys_JSON = ['id', 'bank_account_detail_uuid']\n\nparquet_definition = {'id':'bigint','user_uuid':'string','account_sr_no':'bigint','bank_account_detail_uuid':'string','credit_card_uuid':'string',\n                    'card_reference_id':'string','transfer_frequency':'string','transfer_date':'bigint','transfer_end_date':'bigint',\n                    'transfer_count':'bigint','transfer_id':'bigint','transfer_result_type':'string','transfer_status':'string',\n                    'response_code':'string','response_desc':'string','trans_id':'string','created_date':'bigint','modified_date':'bigint',\n                    'recurrence_id':'string','created_timestamp':'date','modified_timestamp':'date','json_data':'string'}\n\ndf_schema = list(parquet_definition.keys())\ndf_schema.extend(partition_cols)\n\n# Any calculated columns, process here\n# Bring these column values from the JSON log storage\n# None\nparquet_schema = list(parquet_definition.keys())\nparquet_schema.extend(partition_cols)\n\n","cell_type":"code","execution_count":24},{"metadata":{},"outputs":[],"source":"# cell updated on 09-03-2023 : json merging.\nenv = 'dev'\ndb_name = 'onboarding_dev'\ntable_name = 'finicity_customer'\nbucket_name = 'neumoney-stagging-thp-logs'\njson_col_name = 'json_data'\n\nlst_json = s3obj.json_file_prefix(df,env,db_name,table_name,json_col_name,merge_keys_JSON,bucket_name)\n","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"json_df = pd.DataFrame(lst_json)\ndf1 = pd.merge(df,json_df, how=\"left\", on=merge_keys_JSON)\n\ndf_new = df1[df_schema].copy()\n# [....,json_data,user_uuid]\n","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"# Convert column data types in the delta dataframe, inorder to match the data types of the existing parquet file\n# If the delta dataframe's data types for merge keys are not matching with parquet file's data types, while merging these two (inorder to remove duplicates), will not find matching records and will cause duplicates\npandas_type_map = map_parquet_types_to_pandas(merge_keys, parquet_definition)\nprint(pandas_type_map)\n\n# Convert pandas dataframe column types using the map\ndf = df.astype(pandas_type_map)","cell_type":"code","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"# Merge the delta records fetched from the explore object with existing parquet file in S3, so that every affected partition is reconstructed in the dataframe. \n# Note: Overlapping partitions will be overwritten in S3.\n\nfinal_df = mergeDeltaWithExistingParquet(df, file_prefix, partition_cols, merge_keys, df_schema, s3obj)\n#print(final_df.shape)","cell_type":"code","execution_count":25},{"metadata":{"trusted":true},"outputs":[],"source":"# # Write the data into parquet, with the parquet schema.\nresp = write2Parquet(final_df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table)\nprint(resp)","cell_type":"code","execution_count":26},{"metadata":{"trusted":true},"outputs":[],"source":"# Update batch control record for the batch job with the latest time stamp\nif resp.get(\"status\") and resp.get(\"records_written\") > 0:\n    #updateBatchControl(final_df, timestamp_column, batch_code, rdb)\n    print(f\"Updated batch_control: {batch_code}\")\nelse:\n    print(f\"No records written. batch_control not updated\")","cell_type":"code","execution_count":16},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.10.9","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":2}