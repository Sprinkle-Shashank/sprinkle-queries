{"cells":[{"metadata":{"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"outputs":[],"source":"!pip install awswrangler","cell_type":"code","execution_count":1},{"metadata":{"trusted":true},"outputs":[],"source":"import pandas as pd\nfrom sprinkleSdk import SprinkleSdk as sp\nimport boto3\nimport awswrangler as wr\nfrom datetime import datetime\nimport logging\nimport json\nimport redshift_connector","cell_type":"code","execution_count":2},{"metadata":{"trusted":true},"outputs":[],"source":"def getBoto3Session(aws_access_key, aws_secret_access_key, region):\n    boto3session = None\n    try:\n        boto3session = boto3.Session(aws_access_key_id=aws_access_key,\n                                aws_secret_access_key=aws_secret_access_key,\n                                region_name=region)\n    except Exception as e:\n        print(f\"getBoto3Session: {str(e)}\")\n    return boto3session\n\ndef getEnvConfig(env=None):\n    # Specify the AWS user's access key/secret key here.\n    AWS_ACCESS_KEY_ID = \"AKIAVMM664F75NS4NJOL\"\n    AWS_SECRET_ACCESS_KEY = \"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\"\n    AWS_REGION_NAME = 'us-east-1'\n    try:\n\n        botoSession = getBoto3Session(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME)\n        if not botoSession:\n            raise Exception(\"Failed to obtain boto session\")\n\n        if not env or env == 'DEV':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": \"neumoney-dev-redshift-cluster.crwu8dyj5u4p.us-east-1.redshift.amazonaws.com\",\n                \"REDSHIFT_PORT\": 5439,\n                \"REDSHIFT_DATABASE\": \"neumoney-dev\",\n                \"REDSHIFT_USER\": \"neumoney\",\n                \"REDSHIFT_PWD\": \"p+6JVumb>g)QkiV\"\n            }\n        if env == 'PROD':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": wr.secretsmanager.get_secret(\"REDSHIFT_HOST\", botoSession),\n                \"REDSHIFT_PORT\": wr.secretsmanager.get_secret(\"REDSHIFT_PORT\", botoSession),\n                \"REDSHIFT_DATABASE\": wr.secretsmanager.get_secret(\"REDSHIFT_DATABASE\", botoSession),\n                \"REDSHIFT_USER\": wr.secretsmanager.get_secret(\"REDSHIFT_USER\", botoSession),\n                \"REDSHIFT_PWD\": wr.secretsmanager.get_secret(\"REDSHIFT_PWD\", botoSession)\n            }\n\n    except Exception as e:\n        print(f\"getConfig: {str(e)}\")\n        return None\n    return config","cell_type":"code","execution_count":3},{"metadata":{"trusted":true},"outputs":[],"source":"class AwsS3():\n    def __init__(self, config):\n        self.AWS_ACCESS_KEY_ID = config.get(\"AWS_ACCESS_KEY_ID\")\n        self.AWS_SECRET_ACCESS_KEY = config.get(\"AWS_SECRET_ACCESS_KEY\")\n        self.region_name = config.get(\"AWS_REGION_NAME\")\n        self.bucket_name = config.get(\"BUCKET_NAME\")\n        self.boto3session = None\n        self.s3client = None\n\n    def initialize(self):\n        self.getBoto3Session()\n        self.getBoto3S3Client()\n        return True\n\n    def getS3Url(self, file_prefix):\n        url = f\"s3://{self.bucket_name}/{file_prefix}\"\n        return url\n\n    def getBoto3Session(self):\n        try:\n            session = boto3.Session(aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                                    aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n                                    region_name=self.region_name)\n            self.boto3session = session\n        except Exception as e:\n            print(f\"getBoto3Session: {str(e)}\")\n            return False\n        return True\n\n    def getBoto3S3Client(self):\n        try:\n            s3_client = boto3.client('s3', aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                                     aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY, region_name=self.region_name)\n            self.s3client = s3_client\n        except Exception as e:\n            print(f\"getBoto3S3Client: {str(e)}\")\n            return False\n        return True\n\n    def existS3Key(self, prefix):\n        found = False\n        try:\n            result = self.s3client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)\n            # result = self.s3client.list_objects_v2(Bucket='neumoney-stagging-thp-log', Prefix='thp-logs/22-02-2023/credit-card/uat/')\n            if 'Contents' in result.keys():\n                found = True\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return found\n\n    def existS3Key2(self, prefix,bucket_name):\n        found = False\n        try:\n            result = self.s3client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n            if 'Contents' in result.keys():\n                found = True\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return found\n\n\n    def json_file_prefix(self, df, env, db_name, table_name,json_col_name,merge_keys_json,bucket_name):\n        json_list = []\n        for index, row in df.iterrows():\n            row_json = dict()\n            file_prefix_json = '{}/{}/{}/'.format(env, db_name, table_name)\n            file_prefix_json_full = '{}/{}/{}/{}_{}'.format(env, db_name, table_name,table_name, row[merge_keys_JSON[0]])\n            # file_prefix_json = 'dev/credit_limit_dev/scienaptic_details/'\n            # file_prefix_json_full = 'dev/credit_limit_dev/scienaptic_details/scienaptic_details_30dcaaa2-b8c6-11ed-b4bc-a35090bde0c0_T_2023_03_02_06_49_28_318431.json'\n\n            Path_list = self.s3client.list_objects_v2(Bucket=bucket_name, Prefix=file_prefix_json)\n            if 'Contents' in Path_list.keys():\n                found = True\n            else:\n                found = False\n            if found:\n                particular_path = [item['Key'] for item in Path_list['Contents'] if\n                                   item['Key'].startswith(file_prefix_json_full)]\n                result = sorted(particular_path, reverse=True)\n                if not result:\n                    result = str()\n                else:\n                    result = str(result[0])\n\n                json_data = s3obj.readJSON(result,bucket_name)\n                if json_data['status'] == 'ok':\n                    stud_obj = json.loads(json_data['data'])\n                    row_json[json_col_name] = str(stud_obj)\n                    for ln in merge_keys_json:\n                        row_json[ln] = row[ln]\n\n                else:\n                    row_json[json_col_name] = str()\n                    for ln in merge_keys_json:\n                        row_json[ln] = row[ln]\n            else:\n                row_json[json_col_name] = str()\n                for ln in merge_keys_json:\n                    row_json[ln] = row[ln]\n            json_list.append(row_json)\n        return json_list\n\n\n    def fetchS3Key(self, prefix):\n        try:\n            Path_list = self.s3client.list_objects_v2(Bucket='neumoney-stagging-thp-logs', Prefix='thp-logs/22-02-2023/credit-card/uat/')\n            particular_path = [item['Key'] for item in Path_list['Contents'] if item['Key'].startswith(\n                'thp-logs/22-02-2023/credit-card/uat/credit_card_0e9884e2-b1f1-11ed-bcde-ebc0e70de44d')]\n            result = sorted(particular_path,reverse=True)\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return result[0]\n\n    def writeParquet(self, df, file_prefix, partition_cols, parquet_definition, glue_database, glue_table):\n        \"\"\"\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html\n        file_prefix: the prefix for the table. E.g: \"s3://neu-data-analytics/application_data/application_data_identity/user_data_v0\"\n        partition_cols: List of columns by which the parquet is to be partitioned\n        \"\"\"\n        resp = {}\n        try:\n            if df.empty:\n                raise Exception(\"Dataframe empty. Nothing to write to S3.\")\n\n            partitioned = True if partition_cols else False\n            s3url = self.getS3Url(file_prefix)\n            file_exists = self.existS3Key(file_prefix)\n\n            if file_exists:\n                # logging.info(f\"URL:{s3url} exists. Overwriting partitions...\")\n                print(logging.info(f\"URL:{s3url} exists. Overwriting partitions...\"))\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        mode='overwrite_partitions',\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition\n                                        )\n            else:\n                logging.info(f\"URL:{s3url} does not exist. Creating...\")\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition)\n\n            if not resp:\n                raise Exception(f'Failed to write parquet at: {s3url}')\n\n        except Exception as e:\n            print(f\"writeParquet: {str(e)}\")\n            return resp\n        return resp\n\n    def readParquet(self, file_prefix, partition_predicate):\n        \"\"\"\n        Reads entire parquet dataset if partition_predicate is None, If supplied, reads one partition. Returns a pandas dataframe.\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.read_parquet.html\n        ::file_prefix: Prefix for the table. E.g: \"s3://neu-data-analytics/persona_data/inquiry_data\"\n        ::partition_predicate: partition string: E.g: \"<column_name1>=<value>/<column_name2>=<value>/....\" format. Depends on the number of levels of partitioning\n        \"\"\"\n        df = pd.DataFrame()\n        urls = []\n        try:\n            # Read each partition from the supplied list.\n            s3url = self.getS3Url(file_prefix)\n\n            # If there are partitions specified, read only those specific partitions. Else read the entire parquet file\n            if partition_predicate:\n                url = f\"{s3url}/{partition_predicate}\"\n            else:\n                url = s3url\n\n            print(url)\n            df = wr.s3.read_parquet(path=url, dataset=True, boto3_session=self.boto3session, ignore_empty=True,\n                                    ignore_index=True, path_suffix=[\".snappy.parquet\"])\n            # print(\"DF type is this \",type(df),)\n        except Exception as e:\n            print(f\"readParquet: {str(e)}\")\n        return df\n\n    def readJSON(self, file_prefix,bucket_name):\n        \"\"\"\n        Reads a JSON file with the specified file_prefix from the S3 bucket.\n        Returns a dictionary of the format {\"data\": {..json data read from s3 file}, \"status\": \"ok|FileNotFound|error|...\"}\n        \"\"\"\n        resp = {\"data\": None, \"status\": None}\n        try:\n            if self.existS3Key2(file_prefix,bucket_name):\n                result = self.s3client.get_object(Bucket=bucket_name, Key=file_prefix)\n                if not result:\n                    raise Exception(\"ReadError\")\n                jsonbody = result.get(\"Body\").read().decode()\n                if not jsonbody:\n                    raise Exception(\"EmptyJson\")\n                resp = {\"data\": jsonbody, \"status\": \"ok\"}\n            else:\n                raise Exception(\"FileNotFound\")\n        except Exception as e:\n            resp = {\"status\": str(e), \"data\": None}\n            print(f\"readJSON: {str(e)}\")\n        return resp\n","cell_type":"code","execution_count":4},{"metadata":{"trusted":true},"outputs":[],"source":"\nclass RedshiftDb(object):\n    def __init__(self, config):\n        self.env = config.get(\"env\")\n        self.logger = logging.getLogger(\"RedshiftDb\")\n        self.dbConn = self._get_db_connection(config)\n\n    def _get_db_connection(self, config):\n        dbConn = None\n        try:\n            dbConn = redshift_connector.connect(host=config.get(\"REDSHIFT_HOST\"),\n                                                port=config.get(\"REDSHIFT_PORT\"),\n                                                database=config.get(\"REDSHIFT_DATABASE\"),\n                                                user=config.get(\"REDSHIFT_USER\"),\n                                                password=config.get(\"REDSHIFT_PWD\")\n                                                )\n            if not dbConn:\n                raise Exception(\"Could not connect\")\n        except Exception as e:\n            self.logger.error(f\"connect: {str(e)}\")\n        return dbConn\n\n\n    def fetchAll(self, sql, params=None):\n        \"\"\"\n        Takes a SQL string optional list of parameters as input\n        Returns pandas dataframe if the query was successful and has records.\n        On error or when result is empty, returns a None\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            cursor.execute(sql, params)\n            result = cursor.fetch_dataframe()\n\n        except Exception as e:\n            self.logger.error(f\"fetchall: {str(e)}\")\n        return result\n\n\n    def execute(self, sql, params=None):\n        \"\"\"\n        Executes one sql with an optional list of parameters\n        Returns number of records affected, when the SQL is successful. None in case of errors\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            res = cursor.execute(sql, params)\n            self.dbConn.commit()\n            result = res.rowcount\n\n        except Exception as e:\n            self.logger.error(f\"execute: {str(e)}\")\n            if self.dbConn:\n                self.dbConn.rollback()\n        return result\n","cell_type":"code","execution_count":5},{"metadata":{"trusted":true},"outputs":[],"source":"\n# Write the final dataframe to parquet\ndef write2Parquet(df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database,\n                  glue_table):\n    res = {}\n    try:\n        # Write the prepared dataset back into S3. \"overwrite_partitions\" mode is being used while writing into an existing parquet file.\n        # This will ensure that, if there is a record that needs to go into an existing S3 partition, then that entire partition is overwritten.\n        # The step above makes sure that the dataset being overwritten contains all records that must be present in the target partition.\n        if not df.empty:\n            df_final = df[parquet_schema].copy()  # Selecting columns in the correct order\n            print(\"write2Parquet: Writing parquet...\")\n            resp = s3obj.writeParquet(df_final, file_prefix, partition_cols, parquet_definition, glue_database,\n                                      glue_table)\n            if not resp:\n                raise Exception(f\"Failed to write into [{file_prefix}] parquet.\")\n            res = {\"status\": True, \"records_written\": df.shape[0]}\n        else:\n            res = {\"status\": True, \"records_written\": 0}\n            print(\"write2Parquet: Final delta record set is empty. Nothing to write to S3.\")\n    except Exception as e:\n        print(f\"write2Parquet: {str(e)}\")\n        res = {\"status\": False, \"records_written\": 0}\n    return res\n\n\ndef map_parquet_types_to_pandas(merge_keys, parquet_definition):\n    type_map = None\n    try:\n        type_map = {}\n        for col in merge_keys:\n            parquet_type = parquet_definition.get(col)\n            pandas_type = \"string\"\n            if parquet_type in ['bigint', 'integer']:\n                pandas_type = \"Int64\"\n            if parquet_type in ['double']:\n                pandas_type = \"float64\"\n            if parquet_type in ['date']:\n                pandas_type = \"datetime64\"\n                # Add more if needed\n\n            type_map[col] = pandas_type\n    except Exception as e:\n        print(f\"map_parquet_types_to_pandas: {str(e)}\")\n    return type_map","cell_type":"code","execution_count":6},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"def getUniquePartitionValues(df):\n    return df[[\"part_appcreated_month\",\"part_appcreated_date\"]].drop_duplicates().to_dict('records')","cell_type":"code","execution_count":7},{"metadata":{"trusted":true},"outputs":[],"source":"def fetchExistingPartitions(parts_list, file_prefix, df_schema):\n    \"\"\"\n    Given a list of partition predicates, read all those partitions and concatenate them into a single pandas dataframe\n    Each item in parts_list will be of the type: {\"part_appcreated_month\": \"202210\", \"part_appcreated_date\": \"20221005\"}\n    \"\"\"\n    part_dfs = []\n    existing_data_df = pd.DataFrame()\n    try:\n        for partition_predicate in parts_list:\n            df_part = s3obj.readParquet(file_prefix, f\"part_appcreated_month={partition_predicate.get('part_appcreated_month')}/part_appcreated_date={partition_predicate.get('part_appcreated_date')}\")\n            \n            if not df_part.empty:\n                df_part[\"part_appcreated_month\"] = partition_predicate.get('part_appcreated_month')\n                df_part[\"part_appcreated_date\"] = partition_predicate.get('part_appcreated_date')\n                part_dfs.append(df_part)\n            \n        if part_dfs:\n            existing_data_df = pd.concat(part_dfs)[df_schema] # Return only the columns which will be present in the incoming delta dataset. Strip all derived columns existing in parquet \n    except Exception as e:\n        print(f\"fetchExistingPartitions: {str(e)}\")\n    return existing_data_df","cell_type":"code","execution_count":8},{"metadata":{"trusted":true},"outputs":[],"source":"def getValueByPath(json_str, path):\n    \"\"\"\n    json_str: Json string from which key value has to be extracted. Function converts the JSON string into dictionary internally.\n    path: The path for the key value. Each element is spearated by |. \n          E.g 1: The path \"address|zip\" will fetch \"56789\" from json: {\"address\": {\"zip\": \"56789\", \"city\":\"xyz\"}}\n          E.g 2: The path \"employees#1|name\" will fetch \"abc\" from json: {\"employees\":[{\"name\":\"xyz\", \"age\":34}, {\"name\":\"abc\", \"age\":30}, {\"name\":\"lmnop\", \"age\":31}]}\n    \"\"\"\n    val = None\n    try:\n        if not path:\n            raise Exception\n        if not json_str:\n            raise Exception\n            \n        path_elements = path.split(\"|\")\n        obj = json.loads(json_str)\n        for element in path_elements:\n            node_name_splits = element.split('#') # Incase, the element name contains the array index\n            node_name = node_name_splits[0]\n            array_index = int(node_name_splits[1]) if len(node_name_splits) > 1 else -1\n            \n            if array_index < 0:\n                obj = obj.get(node_name)\n            else:\n                obj = obj.get(node_name)[array_index]\n            \n        val = obj    \n    except Exception as e:\n        pass\n    return val","cell_type":"code","execution_count":9},{"metadata":{"trusted":true},"outputs":[],"source":"def mergeDeltaWithExistingParquet(df, file_prefix, partition_cols, merge_keys, df_schema, s3obj):\n    \"\"\"\n    ::df: dataframe having delta records for the batch\n    ::file_prefix: The parquet file name. E.g: persona_inquiry_data\n    ::partition_cols: Columns used as partition keys. Must be a list, column names in the order of partition hierarchy. E.g: [\"part_appcreated_month\", \"part_appcreated_date\"]\n    ::merge_keys: Unique identifying columns that can be used to join delta dataframe with existing parquet data, inorder to identify update candidates\n    ::final_schema: Final list of columns to be stored into parquet, in the exact order.\n    \"\"\"\n    if df.empty:\n        print(\"mergeDeltaWithExistingParquet: Delta records dataset is empty. Nothing to merge.\")\n        return df\n    \n    df_final = pd.DataFrame()\n    records_written = 0\n    try:\n        parquet_exists = s3obj.existS3Key(file_prefix)\n        \n        if not parquet_exists:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Does not exist.\")\n            df_final = df\n        else:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Exists. Preparing delta\")\n            \n            delta_partitions_list = getUniquePartitionValues(df)\n            existing_data_df = fetchExistingPartitions(delta_partitions_list, file_prefix, df_schema)\n    \n            # Left join existing records(in S3) with the incoming delta records. Use the columns that uniquely identifies a record\n            if not existing_data_df.empty:\n                # get only the merge columns from the delta set - just to do a left join with existing data\n                df_delta_filter = df[merge_keys].copy(deep=True).reset_index(drop=True)\n                df_delta_filter[\"new_delta\"] = 1\n                \n                # Join with the incoming delta set and get the new_delta column =1, if the record exists in the incoming delta set\n                existing_marked_df = existing_data_df.merge(df_delta_filter, how=\"left\", on=merge_keys).copy(deep=True)\n    \n                # Filter out records from existing S3 records, which found a match in the incoming delta set.\n                df_existing_portion = existing_marked_df.query(\"~(new_delta==1)\").copy(deep=True)\n                df_existing_portion[\"new_delta\"] = 0\n\n                df[\"new_delta\"] = 1 # Just to match the list of columns\n\n                # Now concatenate the incoming delta set (full), with those existing rows from S3 which does not exist in the new delta set.\n                # This makes the full record set for the target.\n                df_final = pd.concat([df_existing_portion, df])[df_schema]\n            else:\n                df_final = df\n\n    except Exception as e:\n        print(f\"mergeDeltaWithExistingParquet: {str(e)}\")\n    return df_final","cell_type":"code","execution_count":10},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"#Connection\nprint(f\"Starting Batch Job: Incremental dump of scienaptic table data into Parquet\")\nconfig = getEnvConfig(env=\"DEV\") # Change env=\"PROD\" while deploying to production\nif not config:\n    print(f\"ERROR: Could not get credential configurations.\")\n    ","cell_type":"code","execution_count":11},{"metadata":{"trusted":true},"outputs":[],"source":"config = getEnvConfig(env=\"DEV\")\ns3obj = AwsS3(config)\ns3obj.initialize()\n\n#rdb = RedshiftDb(config)  # <<<<<---- Uncomment once Sprinkle moves to neumoney n/w\nbatch_code = \"SCIENAPTIC_JSON\"\ntimestamp_column = \"modified_date\"\n\nglue_database = \"archive_redshift_db\"\nglue_table = \"credit_limit_scienaptic_details\"\n\nfile_prefix = \"application_data/credit_limit/scienaptic_data_v0\"\npartition_cols = [\"part_appcreated_month\", \"part_appcreated_date\"]\n\n# Columns used to identify unique records from the data set\nmerge_keys = ['application_id','scienaptic_uuid','part_appcreated_month','part_appcreated_date']\n# merge_keys_JSON[0] shauld be primary key and merge_keys_JSON[1] shauld be created date\nmerge_keys_JSON = ['application_uuid','scienaptic_uuid','part_appcreated_month','part_appcreated_date']\n","cell_type":"code","execution_count":12},{"metadata":{"trusted":true},"outputs":[],"source":"# Read the delta records from scienaptic tables, using the explore object. (Explore name: scienaptic_detail_data)\nexplore_id = '902fcc373d1f49db9c811105b2921534' #scienaptic_detail_data\ndf = sp.read_explore(explore_id)\n\nsql = (\"select tab.* from \"\n    \"( select  \"\n    \" apps.id as application_id,\"\n    \" stg.id as scienaptic_id,\"\n    \" stg.scienaptic_uuid,\"\n    \" stg.application_uuid,\"\n    \" stg.product_uuid,\"\n    \" stg.user_id,\"\n    \" stg.decision,\"\n    \" stg.created_at,\"\n    \" stg.created_date,\"\n    \" stg.modified_date,\"\n    \" stg.credit_score,\"\n    \" stg.credit_limit,\"\n    \" stg.applied_credit_limit,\"\n    \" TIMESTAMP 'epoch' + stg.modified_date/1000 *INTERVAL '1 second' as modified_timestamp, \"\n    \" TIMESTAMP 'epoch' + apps.created_date/1000 *INTERVAL '1 second' as application_created_date,\"\n    \" current_timestamp as batch_timestamp  \"\n    \" from \"\n    \" stage.ds_postgres_credit_limit_scienaptic_details_redshift_scienaptic_details_stg stg \"\n    \" left join stage.ds_postgres_onboarding_application_redshift_application_stg apps on stg.application_uuid=apps.application_uuid \"\n    \" ) tab \"\n    \" join stage.batch_control bc on tab.modified_timestamp >= bc.last_timestamp and batch_code = %s \"\n    )\nsql_params = [batch_code]\n# Uncomment once Sprinkle has moved to neumoney network\n# Fetch data from stage table in redshift\n# df = rdb.fetchAll(sql, sql_params)\n\n\n# Convert the application created datetime into YYYYMMDD format for partitioning key\ndf[\"application_created_date\"] = df[\"application_created_date\"].fillna('1970-01-01T0:0:0.0+00:00')\ndf[\"part_appcreated_month\"] = df[\"application_created_date\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m\"))  \ndf[\"part_appcreated_date\"] = df[\"application_created_date\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m%d\"))  \n\nprint(f\"Fetched {df.shape[0]} records to be added into [{file_prefix}] S3 file\")","cell_type":"code","execution_count":13},{"metadata":{"trusted":true},"outputs":[],"source":"\nparquet_definition = {'application_id':'bigint','scienaptic_id':'bigint','scienaptic_uuid':'string','application_uuid':'string',\n                    'product_uuid':'string','user_id':'string','decision':'string','created_at':'bigint','created_date':'bigint',\n                    'modified_date':'bigint','credit_score':'double','credit_limit':'double','applied_credit_limit':'double',\n                    'modified_timestamp':'date','application_created_date':'date','batch_timestamp':'date' , 'scienaptic_data': 'string'#,'json_data':'string'\n                    }\n\n\n\ndf_schema = list(parquet_definition.keys())\ndf_schema.extend(partition_cols)\n\n# Any calculated columns, process here\n# Bring these column values from the JSON log storage\n# None\n\nparquet_schema = list(parquet_definition.keys())\nparquet_schema.extend(partition_cols)\n\n# Convert column data types in the delta dataframe, inorder to match the data types of the existing parquet file\n# If the delta dataframe's data types for merge keys are not matching with parquet file's data types, while merging these two (inorder to remove duplicates), will not find matching records and will cause duplicates\npandas_type_map = map_parquet_types_to_pandas(merge_keys, parquet_definition)\nprint(pandas_type_map)\n\n# Convert pandas dataframe column types using the map\ndf = df.astype(pandas_type_map)\n\n","cell_type":"code","execution_count":14},{"metadata":{"trusted":true},"outputs":[],"source":"env = 'dev'\ndb_name = 'credit_limit_dev'\ntable_name = 'scienaptic_details'\nbucket_name = 'neumoney-stagging-thp-logs'\njson_col_name = 'scienaptic_data'\n\n\njson_path = s3obj.json_file_prefix(df,env,db_name,table_name,json_col_name,merge_keys_JSON,bucket_name)\n\njson_df = pd.DataFrame(json_path)\ndf1 = pd.merge(df,json_df, how=\"left\", on=merge_keys_JSON)","cell_type":"code","execution_count":15},{"metadata":{"trusted":true},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"df_new = df1[df_schema].copy()\n# [....,json_data, part_appcreated_month, part_appcreated_date]\n\n# Convert column data types in the delta dataframe, inorder to match the data types of the existing parquet file\n# If the delta dataframe's data types for merge keys are not matching with parquet file's data types, while merging these two (inorder to remove duplicates), will not find matching records and will cause duplicates\npandas_type_map = map_parquet_types_to_pandas(merge_keys, parquet_definition)\nprint(pandas_type_map)\n\n# Convert pandas dataframe column types using the map\ndf_new = df_new.astype(pandas_type_map)","cell_type":"code","execution_count":16},{"metadata":{"trusted":true},"outputs":[],"source":"# Merge the delta records fetched from the explore object with existing parquet file in S3, so that every affected partition is reconstructed in the dataframe. \n# Note: Overlapping partitions will be overwritten in S3.\n\nfinal_df = mergeDeltaWithExistingParquet(df_new, file_prefix, partition_cols, merge_keys, df_schema, s3obj)\nprint(final_df.shape)","cell_type":"code","execution_count":17},{"metadata":{"trusted":true},"outputs":[],"source":"# # Write the data into parquet, with the parquet schema.\nresp = write2Parquet(final_df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table)\nprint(resp)","cell_type":"code","execution_count":18},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.10.9","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":2}