{"cells":[{"metadata":{},"outputs":[],"source":"Final version, this script is written under the assumption that the script will only run once in a day and collect the downloads data and populate in the table","cell_type":"markdown","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"!pip3 install pyjwt --upgrade","cell_type":"code","execution_count":1},{"metadata":{"trusted":true},"outputs":[],"source":"!pip3 install cryptography --upgrade","cell_type":"code","execution_count":2},{"metadata":{"trusted":true},"outputs":[],"source":"import os\nimport csv\nimport jwt\nimport gzip\n#import boto3\nimport requests\nimport pandas as pd\nfrom io import StringIO\nfrom IPython.display import display\nfrom sprinkleSdk import SprinkleSdk as sp        # importing sprinkle sdk to intract with sprinkle and redshift.\nfrom datetime import date, datetime, timezone\n\nclass Daily_Download_Counts():\n    \n    def __init__(self):\n        \n        ''' initalizing the values '''\n        \n        #################\n        ## credentials ##\n        #################\n        \n        self.issuer_id      = \"69a6de75-0dc3-47e3-e053-5b8c7c11a4d1\"\n        self.key_id         = \"59W873K3RD\"\n        self.YOUR_VENDOR_ID = 85409565\n        self.sku            = 'LSSTW09082016'\n\n        # reading the secret key\n        self.generated_private_key = \"-----BEGIN PRIVATE KEY-----\\nMIGTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBHkwdwIBAQQgPStV4OhfUk46ow6k\\nIa086H1l6rOW70DT0BdtXp/uMP2gCgYIKoZIzj0DAQehRANCAAT1Q0MljT0/NYsb\\nZRLYLB8wq6hO4Lla/9O8vCIFLJPnIJ48DHfuvPks0i+qV1AuCUOMfkgb/FtK/R+B\\nRvU+tw2G\\n-----END PRIVATE KEY-----\"\n                \n        ###############\n        ## variables ##\n        ###############\n\n        self.frequency     = 'DAILY'\n        self.reportSubType = 'SUMMARY'\n        self.reportType    = 'SALES'\n        self.today_date    = date.today() # get current date\n        self.reportDate    = self.today_date #'2022-06-20'\n        self.base_dir      = os.getcwd()  # save path\n        self.explore_id    = '41299eba828b41e292b2ab771a7d33e2'\n        \n        \n        \n    def sign_appstore_token(self):\n        \n        ''' function for generating JWT tocken from the secret access key'''\n        \n        bin_private_key = self.generated_private_key.encode()\n        current_unix = int(datetime.now(tz=timezone.utc).timestamp())\n        token = jwt.encode({\n                \"iss\": self.issuer_id,\n                \"iat\": current_unix,\n                \"exp\": current_unix + 1000, \n                \"aud\": \"appstoreconnect-v1\",\n        }, key= bin_private_key, algorithm= 'ES256', headers= {\n            \"alg\": \"ES256\",\n            \"kid\": self.key_id,\n            \"typ\": \"JWT\"\n        })\n        \n        return token\n\n    \n    def Get_Download_Counts(self,token):\n        \n        url = f\"https://api.appstoreconnect.apple.com/v1/salesReports?filter[frequency]={self.frequency}&filter[reportDate]={self.reportDate}&filter[reportSubType]={self.reportSubType}&filter[reportType]={self.reportType}&filter[vendorNumber]={self.YOUR_VENDOR_ID}\"\n\n        payload={}\n        headers = {\n          'Authorization': f'Bearer {token}'\n        }\n\n        response = requests.request(\"GET\", url, headers=headers, data=payload)\n        # checking the status code \n        response_code = response.status_code\n        print(f\"response status code : {response_code}\")\n        \n        \n        # if we receive a response with data sucessfully then we procced to convert the response to csv / parquet file.\n        if response_code == 200:\n            \n            # decompressing the response content.\n            result = gzip.decompress(response.content).decode('utf-8')\n            # reading the data from the memory and converting to dataframe.\n            new_data = self.convert_to_DF(result)\n            # filter the new api response data from apple store api- using pandas \n            self.new_data = new_data[new_data['SKU'] == self.sku]\n\n            print('new_data')\n            display(self.new_data)\n            return self.new_data\n        else: \n            self.new_data = None\n            print(\"No new data found\")\n        \n        \n    def convert_to_DF(self,result):\n        \n        ''' function to read the response from the api and convert it into a data frame\n            :param  result : decompressed response content from api call.\n            :return df : dataframe object. \n        '''\n        \n        df = pd.read_csv(StringIO(result),sep='\\t')\n        # adding column for total download counts.\n        # df['total_downloads'] = df['Units'].sum()\n        return df \n    \n    def read_table_data(self):\n        # creating an explore object to create a table in the redshift.\n        self.old_data = sp.read_explore(self.explore_id)\n        \n        \n        if isinstance(self.old_data,pd.DataFrame): \n            #if there is any data in self.old_data, i.e if the table exists then do the following steps.\n            \n            self.old_data = self.old_data.rename(columns={'provider': 'Provider', \n                                          'provider_country'        : 'Provider Country', \n                                          'sku'                     : 'SKU',\n                                          'developer'               : 'Developer',\n                                          'title'                   : 'Title',\n                                          'version'                 : 'Version',\n                                          'product_type_identifier' : 'Product Type Identifier',\n                                          'units'                   : 'Units',\n                                          'developer_proceeds'      : 'Developer Proceeds',\n                                          'begin_date'              : 'Begin Date',\n                                          'end_date'                : 'End Date',\n                                          'customer_currency'       : 'Customer Currency',\n                                          'country_code'            : 'Country Code',\n                                          'currency_of_proceeds'    : 'Currency of Proceeds',\n                                          'apple_identifier'        : 'Apple Identifier',\n                                          'customer_price'          : 'Customer Price',\n                                          'promo_code'              : 'Promo Code',\n                                          'parent_identifier'       : 'Parent Identifier',\n                                          'subscription'            : 'Subscription',\n                                          'period'                  : 'Period',\n                                          'category'                : 'Category',\n                                          'cmb'                     : 'CMB',\n                                          'device'                  : 'Device',\n                                          'supported_platforms'     : 'Supported Platforms',\n                                          'proceeds_reason'         : 'Proceeds Reason',\n                                          'preserved_pricing'       : 'Preserved Pricing', \n                                          'client'                  : 'Client',\n                                          'order_type'              : 'Order Type'\n                                                          })\n            print('old_data')\n            display(self.old_data)\n            return self.old_data\n        \n        else:\n            print('old-table-data (target table) doesnt exists')\n        \n    def mergeDF(self,old_df,new_df):\n        ''' func for merging old table with new table data into the table. '''\n        \n        # only merge if there are two dataframes present.\n        if isinstance(self.old_data,pd.DataFrame) and isinstance(self.new_data,pd.DataFrame) :\n                        \n            # merge 2 tables : already existing table and new table.\n            concat_df = pd.concat([old_df.reset_index(drop=True),new_df.reset_index(drop=True)],ignore_index=True)\n            print('concatnated df data')\n            display(concat_df)\n            return concat_df\n        else:\n            concat_df = None\n            return concat_df\n       \n    def insert_to_table(self,concat_DF):\n        \n        # only insert if there is a concatnated dataframe.\n        if isinstance(concat_DF,pd.DataFrame):\n            \n            # inserting the record into the table.\n            print(\"inserting to table\")\n            #sp.create_or_update_table(\"checking_fileuploads/test_table3\",concat_DF)\n            \n            #sp.createOrUpdateTable('app_store','ds_app_store_daily_downloads', concat_DF)\n            sp.create_or_update_table(\"app_store/daily_downloads\",concat_DF)\n            \n        \ntry :\n    \n    # instantiating the class object\n    Apps_daily_downloads = Daily_Download_Counts()\n\n    # starter code for running the token generator\n    token = Apps_daily_downloads.sign_appstore_token()\n\n    # fetching the download counts\n    new_data = Apps_daily_downloads.Get_Download_Counts(token)\n    \n    \n    # reading the downloads data existing in the data-warehouse table.\n    old_data = Apps_daily_downloads.read_table_data()\n    \n    # merge new-data-df and old-data-df \n    concat_df = Apps_daily_downloads.mergeDF(old_data,new_data)\n    \n    # insert concatnated data frame into the table.\n    Apps_daily_downloads.insert_to_table(concat_df)\n\nexcept Exception as e:\n    print(e)\n    ","cell_type":"code","execution_count":2},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"## version 2:latest :: adding the delete feature if the record already exists with same date","cell_type":"markdown","execution_count":0},{"metadata":{},"outputs":[],"source":"This script is written under the assumption that script will be running multiple times a day to collect hourly downloads count and populate it in a table.","cell_type":"markdown","execution_count":0},{"metadata":{},"outputs":[],"source":"import os\nimport csv\nimport jwt\nimport gzip\n#import boto3\nimport requests\nimport pandas as pd\nfrom io import StringIO\nfrom IPython.display import display\nfrom sprinkleSdk import SprinkleSdk as sp        # importing sprinkle sdk to intract with sprinkle and redshift.\nfrom datetime import date, datetime, timezone\n\nclass Daily_Download_Counts():\n    \n    def __init__(self):\n        \n        ''' initalizing the values '''\n        \n        #################\n        ## credentials ##\n        #################\n        \n        self.issuer_id      = \"69a6de75-0dc3-47e3-e053-5b8c7c11a4d1\"\n        self.key_id         = \"59W873K3RD\"\n        self.YOUR_VENDOR_ID = 85409565\n        self.sku            = 'LSSTW09082016'\n\n        # reading the secret key\n        self.generated_private_key = \"-----BEGIN PRIVATE KEY-----\\nMIGTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBHkwdwIBAQQgPStV4OhfUk46ow6k\\nIa086H1l6rOW70DT0BdtXp/uMP2gCgYIKoZIzj0DAQehRANCAAT1Q0MljT0/NYsb\\nZRLYLB8wq6hO4Lla/9O8vCIFLJPnIJ48DHfuvPks0i+qV1AuCUOMfkgb/FtK/R+B\\nRvU+tw2G\\n-----END PRIVATE KEY-----\"\n                \n        ###############\n        ## variables ##\n        ###############\n\n        self.frequency        = 'DAILY'\n        self.reportSubType    = 'SUMMARY'\n        self.reportType       = 'SALES'\n        self.today_date       = date.today() # get current date\n        self.reportDate       = self.today_date #'2022-06-20'\n        self.base_dir         = os.getcwd()  # save path\n        self.data_explore_id  = '41299eba828b41e292b2ab771a7d33e2' # for collecting the data from target table, collecting old data.\n        seld.batch_explore_id = 'f76192b3337f407ebdd02f4a9ec0c265' # for collecting the last batch run job timestamp. \n        \n        \n    def sign_appstore_token(self):\n        \n        ''' function for generating JWT tocken from the secret access key'''\n        \n        bin_private_key = self.generated_private_key.encode()\n        current_unix = int(datetime.now(tz=timezone.utc).timestamp())\n        token = jwt.encode({\n                \"iss\": self.issuer_id,\n                \"iat\": current_unix,\n                \"exp\": current_unix + 1000, \n                \"aud\": \"appstoreconnect-v1\",\n        }, key= bin_private_key, algorithm= 'ES256', headers= {\n            \"alg\": \"ES256\",\n            \"kid\": self.key_id,\n            \"typ\": \"JWT\"\n        })\n        \n        return token\n    \n    # <-- newly added script from dibin -->\n    def get_months_years_between_dates(self,start_date,end_date):\n        start_date = dt.strptime(start_date, \"%Y-%m-%d\")\n        end_date = dt.strptime(end_date, \"%Y-%m-%d\")\n        months = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month)\n        year_list  = []\n        month_list = []\n        date_list  = []\n        for i in range(months+1):\n            year_list.append(start_date.year + ((start_date.month-1+i)//12))\n            month_list.append((start_date.month-1+i) % 12 + 1)\n        result = list(zip(year_list, month_list))\n        return result\n    \n    # <-- newly added script from dibin -->\n    def load_data(self,files_to_featch):\n        final_df  = pd.DataFrame()\n        for year, month in files_to_featch:\n#           print(type(month))\n            month_string = '{:02d}'.format(month)\n            ym=str(year)+month_string\n            file_path='stats/installs/installs_air.com.bloomerangs.cinemacity_'+ym+'_app_version.csv'\n            creds = Credentials.from_service_account_info({\n                   \"client_email\":self.client_email,\n                    \"private_key\":self.private_keys ,\n                    \"token_uri\": self.token_uri\n                    })\n            service = build('storage', 'v1', credentials=creds)\n#             print(creds)\n#             print(self.client_email)\n#             print(file_path)\n#             print(self.bucket_name)\n            request = service.objects().get_media(bucket=self.bucket_name, object=file_path)\n            response = request.execute()\n            responses = response.decode(encoding=\"UTF-16\")\n            file = io.StringIO(responses)\n            df = pd.read_csv(file, encoding=\"UTF-16\")\n#             print('******************df**************************')\n#             print(df.head(5))\n#             print(len(df))\n#             print(len(df.columns))\n            final_df=pd.concat([final_df, df])\n#         print(\"*****************final_df*************\")\n#         print(final_df)\n        return final_df\n\n  \n    def Get_Download_Counts(self,token):\n        \n        url = f\"https://api.appstoreconnect.apple.com/v1/salesReports?filter[frequency]={self.frequency}&filter[reportDate]={self.reportDate}&filter[reportSubType]={self.reportSubType}&filter[reportType]={self.reportType}&filter[vendorNumber]={self.YOUR_VENDOR_ID}\"\n\n        payload={}\n        headers = {\n          'Authorization': f'Bearer {token}'\n        }\n\n        response = requests.request(\"GET\", url, headers=headers, data=payload)\n        # checking the status code \n        response_code = response.status_code\n        print(f\"response status code : {response_code}\")\n        \n        \n        # if we receive a response with data sucessfully then we procced to convert the response to csv / parquet file.\n        if response_code == 200:\n            \n            # decompressing the response content.\n            result = gzip.decompress(response.content).decode('utf-8')\n            # reading the data from the memory and converting to dataframe.\n            new_data = self.convert_to_DF(result)\n            # filter the new api response data from apple store api- using pandas \n            self.new_data = new_data[new_data['SKU'] == self.sku]\n\n            print('new_data')\n            display(self.new_data)\n            return self.new_data\n        else: \n            self.new_data = None\n            print(\"No new data found\")\n        \n        \n    def convert_to_DF(self,result):\n        \n        ''' function to read the response from the api and convert it into a data frame\n            :param  result : decompressed response content from api call.\n            :return df : dataframe object. \n        '''\n        \n        df = pd.read_csv(StringIO(result),sep='\\t')\n        # adding column for total download counts.\n        # df['total_downloads'] = df['Units'].sum()\n        return df \n    \n    def read_table_data(self):\n        # creating an explore object to create a table in the redshift.\n        self.old_data = sp.read_explore(self.data_explore_id)\n        \n        \n        if isinstance(self.old_data,pd.DataFrame): \n            #if there is any data in self.old_data, i.e if the table exists then do the following steps.\n            \n            self.old_data = self.old_data.rename(columns={'provider': 'Provider', \n                                          'provider_country'        : 'Provider Country', \n                                          'sku'                     : 'SKU',\n                                          'developer'               : 'Developer',\n                                          'title'                   : 'Title',\n                                          'version'                 : 'Version',\n                                          'product_type_identifier' : 'Product Type Identifier',\n                                          'units'                   : 'Units',\n                                          'developer_proceeds'      : 'Developer Proceeds',\n                                          'begin_date'              : 'Begin Date',\n                                          'end_date'                : 'End Date',\n                                          'customer_currency'       : 'Customer Currency',\n                                          'country_code'            : 'Country Code',\n                                          'currency_of_proceeds'    : 'Currency of Proceeds',\n                                          'apple_identifier'        : 'Apple Identifier',\n                                          'customer_price'          : 'Customer Price',\n                                          'promo_code'              : 'Promo Code',\n                                          'parent_identifier'       : 'Parent Identifier',\n                                          'subscription'            : 'Subscription',\n                                          'period'                  : 'Period',\n                                          'category'                : 'Category',\n                                          'cmb'                     : 'CMB',\n                                          'device'                  : 'Device',\n                                          'supported_platforms'     : 'Supported Platforms',\n                                          'proceeds_reason'         : 'Proceeds Reason',\n                                          'preserved_pricing'       : 'Preserved Pricing', \n                                          'client'                  : 'Client',\n                                          'order_type'              : 'Order Type'\n                                                          })\n            print('old_data')\n            display(self.old_data)\n            return self.old_data\n        \n        else:\n            print('old-table-data (target table) doesnt exists')\n        \n    def mergeDF(self,old_df,new_df):\n        ''' func for merging old table with new table data into the table. '''\n        \n        # only merge if there are two dataframes present.\n        if isinstance(self.old_data,pd.DataFrame) and isinstance(self.new_data,pd.DataFrame) :\n                        \n            # merge 2 tables : already existing table and new table.\n            concat_df = pd.concat([old_df.reset_index(drop=True),new_df.reset_index(drop=True)],ignore_index=True)\n            print('concatnated df data')\n            display(concat_df)\n            return concat_df\n        else:\n            concat_df = None\n            return concat_df\n       \n    def insert_to_table(self,concat_DF):\n        \n        # only insert if there is a concatnated dataframe.\n        if isinstance(concat_DF,pd.DataFrame):\n            \n            # inserting the record into the table.\n            print(\"inserting to table\")\n            #sp.create_or_update_table(\"checking_fileuploads/test_table3\",concat_DF)\n            \n            #sp.createOrUpdateTable('app_store','ds_app_store_daily_downloads', concat_DF)\n            sp.create_or_update_table(\"app_store/daily_downloads\",concat_DF)\n            \n        \ntry :\n    \n    # instantiating the class object\n    Apps_daily_downloads = Daily_Download_Counts()\n\n    # starter code for running the token generator\n    token = Apps_daily_downloads.sign_appstore_token()\n\n    # fetching the download counts\n    new_data = Apps_daily_downloads.Get_Download_Counts(token)\n    \n    \n    # reading the downloads data existing in the data-warehouse table.\n    old_data = Apps_daily_downloads.read_table_data()\n    \n    # merge new-data-df and old-data-df \n    concat_df = Apps_daily_downloads.mergeDF(old_data,new_data)\n    \n    # insert concatnated data frame into the table.\n    Apps_daily_downloads.insert_to_table(concat_df)\n\nexcept Exception as e:\n    print(e)\n    ","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"test-space","cell_type":"markdown","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"from datetime import datetime as dt\ndef get_months_years_between_dates(start_date,end_date):\n    start_date = dt.strptime(start_date, \"%Y-%m-%d\")\n    end_date = dt.strptime(end_date, \"%Y-%m-%d\")\n    months = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month)\n    year_list = []\n    month_list = []\n    day_list   = []\n    for i in range(months+1):\n        year_list.append(start_date.year + ((start_date.month-1+i)//12))\n        month_list.append((start_date.month-1+i) % 12 + 1)\n    result = list(zip(year_list, month_list))\n    return result\n\nget_months_years_between_dates('2022-08-01','2023-04-01')","cell_type":"code","execution_count":4},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.10.9","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":2}