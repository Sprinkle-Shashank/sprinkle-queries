{"cells":[{"metadata":{"trusted":true},"outputs":[],"source":"!pip install awswrangler","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"import pandas as pd\nfrom sprinkleSdk import SprinkleSdk as sp\nimport boto3\nimport awswrangler as wr\nfrom datetime import datetime\nimport logging\nimport json\nimport redshift_connector","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"def getBoto3Session(aws_access_key, aws_secret_access_key, region):\n    boto3session = None\n    try:\n        boto3session = boto3.Session(aws_access_key_id=aws_access_key,\n                                aws_secret_access_key=aws_secret_access_key,\n                                region_name=region)\n    except Exception as e:\n        print(f\"getBoto3Session: {str(e)}\")\n    return boto3session\n\ndef getEnvConfig(env=None):\n    # Specify the AWS user's access key/secret key here.\n    AWS_ACCESS_KEY_ID = \"AKIAVMM664F73YXKHU7A\"\n    AWS_SECRET_ACCESS_KEY = \"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\"\n    AWS_REGION_NAME = 'us-east-1'\n    try:\n\n        botoSession = getBoto3Session(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME)\n        if not botoSession:\n            raise Exception(\"Failed to obtain boto session\")\n\n        if not env or env == 'DEV':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": \"neumoney-dev-redshift-cluster.crwu8dyj5u4p.us-east-1.redshift.amazonaws.com\",\n                \"REDSHIFT_PORT\": 5439,\n                \"REDSHIFT_DATABASE\": \"neumoney-dev\",\n                \"REDSHIFT_USER\": \"neumoney\",\n                \"REDSHIFT_PWD\": \"p+6JVumb>g)QkiV\"\n            }\n        if env == 'PROD':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": wr.secretsmanager.get_secret(\"REDSHIFT_HOST\", botoSession),\n                \"REDSHIFT_PORT\": wr.secretsmanager.get_secret(\"REDSHIFT_PORT\", botoSession),\n                \"REDSHIFT_DATABASE\": wr.secretsmanager.get_secret(\"REDSHIFT_DATABASE\", botoSession),\n                \"REDSHIFT_USER\": wr.secretsmanager.get_secret(\"REDSHIFT_USER\", botoSession),\n                \"REDSHIFT_PWD\": wr.secretsmanager.get_secret(\"REDSHIFT_PWD\", botoSession)\n            }\n\n    except Exception as e:\n        print(f\"getConfig: {str(e)}\")\n        return None\n    return config","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"class AwsS3():\n    def __init__(self, config):\n        self.AWS_ACCESS_KEY_ID = \"AKIAVMM664F73YXKHU7A\"\n        self.AWS_SECRET_ACCESS_KEY = \"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\"\n        self.region_name = 'us-east-1'\n        self.bucket_name = \"neu-data-analytics\"\n        self.boto3session = None\n        self.s3client = None\n\n    def initialize(self):\n        self.getBoto3Session()\n        self.getBoto3S3Client()\n        return True\n    \n    def getS3Url(self, file_prefix):\n        url = f\"s3://{self.bucket_name}/{file_prefix}\"\n        return url\n    \n    def getBoto3Session(self):\n        try:\n            session = boto3.Session(aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                                    aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n                                    region_name=self.region_name)\n            self.boto3session = session\n        except Exception as e:\n            print(f\"getBoto3Session: {str(e)}\")\n            return False\n        return True\n\n    \n    def getBoto3S3Client(self):\n        try:\n            s3_client = boto3.client('s3', aws_access_key_id=self.AWS_ACCESS_KEY_ID, aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,region_name=self.region_name)\n            self.s3client = s3_client\n        except Exception as e:\n            print(f\"getBoto3S3Client: {str(e)}\")\n            return False\n        return True        \n        \n    def existS3Key(self, prefix):\n        found = False\n        try:\n            result = self.s3client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)\n            if 'Contents' in result.keys():\n                found = True\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return found      \n    \n    def writeParquet(self, df, file_prefix, partition_cols, parquet_definition, glue_database, glue_table):\n        \"\"\"\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html\n        file_prefix: the prefix for the table. E.g: \"s3://neu-data-analytics/application_data/application_data_identity/user_data_v0\"\n        partition_cols: List of columns by which the parquet is to be partitioned\n        \"\"\"\n        resp = {}\n        try:\n            if df.empty:\n                raise Exception(\"Dataframe empty. Nothing to write to S3.\")\n                \n            partitioned =True if partition_cols else False\n            s3url = self.getS3Url(file_prefix)\n            file_exists = self.existS3Key(file_prefix)\n\n            if file_exists:\n                #logging.info(f\"URL:{s3url} exists. Overwriting partitions...\")\n                print(logging.info(f\"URL:{s3url} exists. Overwriting partitions...\"))\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        mode='overwrite_partitions',\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition\n                                       )\n            else:\n                logging.info(f\"URL:{s3url} does not exist. Creating...\")\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition)\n            \n            if not resp:\n                raise Exception (f'Failed to write parquet at: {s3url}')\n            \n        except Exception as e:\n            print(f\"writeParquet: {str(e)}\")\n            return resp\n        return resp             \n    \n    def readParquet(self, file_prefix, partition_predicate):\n        \"\"\"\n        Reads entire parquet dataset if partition_predicate is None, If supplied, reads one partition. Returns a pandas dataframe.\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.read_parquet.html\n        ::file_prefix: Prefix for the table. E.g: \"s3://neu-data-analytics/persona_data/inquiry_data\" \n        ::partition_predicate: partition string: E.g: \"<column_name1>=<value>/<column_name2>=<value>/....\" format. Depends on the number of levels of partitioning\n        \"\"\"\n        df = pd.DataFrame()\n        urls = []\n        try:\n            # Read each partition from the supplied list. \n            s3url = self.getS3Url(file_prefix)\n            \n            # If there are partitions specified, read only those specific partitions. Else read the entire parquet file\n            if partition_predicate:\n                url = f\"{s3url}/{partition_predicate}\"\n            else:\n                url = s3url\n            \n            print(url)    \n            df = wr.s3.read_parquet(path=url, dataset=True, boto3_session=self.boto3session, ignore_empty=True, ignore_index=True, path_suffix=[\".snappy.parquet\"])   \n                \n        except Exception as e:\n            print(f\"readParquet: {str(e)}\")\n        return df \n    def readJSON(self, file_prefix):\n        \"\"\"\n        Reads a JSON file with the specified file_prefix from the S3 bucket. \n        Returns a dictionary of the format {\"data\": {..json data read from s3 file}, \"status\": \"ok|FileNotFound|error|...\"}\n        \"\"\"\n        resp = {\"data\":None, \"status\":None}\n        try:\n            if self.existS3Key(file_prefix):\n                result = self.s3client.get_object(Bucket=self.bucket_name, Key=file_prefix)\n                if not result:\n                    raise Exception(\"ReadError\")\n                jsonbody = result.get(\"Body\").read().decode()\n                if not jsonbody:\n                    raise Exception(\"EmptyJson\")\n                resp = {\"data\":jsonbody, \"status\":\"ok\"}\n            else:\n                raise Exception(\"FileNotFound\")\n        except Exception as e:\n            resp = {\"status\":str(e), \"data\":None}\n            print(f\"readJSON: {str(e)}\")\n        return resp","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"def getUniquePartitionValues(df):\n    return df[[\"part_created_month\",\"part_created_date\"]].drop_duplicates().to_dict('records')","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"def getJsonData(dataframe,merge_keys_json,json_col_name,json_file_path,table_name):\n    try:\n        json_list = []\n        for index, row in df.iterrows():\n            row_json = dict()        \n            file_prefix_json = '{}/{}_{}_{}.json'.format(json_file_path,table_name,row[merge_keys_json[0]],row[merge_keys_json[1]].replace(':',''))\n#             print(file_prefix_json)\n            json_data = s3obj.readJSON(file_prefix_json)\n            if json_data['status'] == 'ok': \n                stud_obj = json.loads(json_data['data'])\n                row_json[json_col_name] = str(stud_obj)\n            else:\n                row_json[json_col_name] = str()\n            for ln in merge_keys_json:\n                row_json[ln] = row[ln]\n\n            json_list.append(row_json)\n    except Exception as e:\n        print(f\"getJSONData: {str(e)}\")\n    return json_list","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"def fetchExistingPartitions(parts_list, file_prefix, df_schema):\n    \"\"\"\n    Given a list of partition predicates, read all those partitions and concatenate them into a single pandas dataframe\n    Each item in parts_list will be of the type: {\"part_appcreated_month\": \"202210\", \"part_appcreated_date\": \"20221005\"}\n    \"\"\"\n    part_dfs = []\n    existing_data_df = pd.DataFrame()\n    try:\n        for partition_predicate in parts_list:\n            df_part = s3obj.readParquet(file_prefix, f\"part_appcreated_month={partition_predicate.get('part_appcreated_month')}/part_appcreated_date={partition_predicate.get('part_appcreated_date')}\")\n            \n            if not df_part.empty:\n                df_part[\"part_appcreated_month\"] = partition_predicate.get('part_appcreated_month')\n                df_part[\"part_appcreated_date\"] = partition_predicate.get('part_appcreated_date')\n                part_dfs.append(df_part)\n            \n        if part_dfs:\n            existing_data_df = pd.concat(part_dfs)[df_schema] # Return only the columns which will be present in the incoming delta dataset. Strip all derived columns existing in parquet \n    except Exception as e:\n        print(f\"fetchExistingPartitions: {str(e)}\")\n    return existing_data_df","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"def getValueByPath(json_str, path):\n    \"\"\"\n    json_str: Json string from which key value has to be extracted. Function converts the JSON string into dictionary internally.\n    path: The path for the key value. Each element is spearated by |. \n          E.g 1: The path \"address|zip\" will fetch \"56789\" from json: {\"address\": {\"zip\": \"56789\", \"city\":\"xyz\"}}\n          E.g 2: The path \"employees#1|name\" will fetch \"abc\" from json: {\"employees\":[{\"name\":\"xyz\", \"age\":34}, {\"name\":\"abc\", \"age\":30}, {\"name\":\"lmnop\", \"age\":31}]}\n    \"\"\"\n    val = None\n    try:\n        if not path:\n            raise Exception\n        if not json_str:\n            raise Exception\n            \n        path_elements = path.split(\"|\")\n        obj = json.loads(json_str)\n        for element in path_elements:\n            node_name_splits = element.split('#') # Incase, the element name contains the array index\n            node_name = node_name_splits[0]\n            array_index = int(node_name_splits[1]) if len(node_name_splits) > 1 else -1\n            \n            if array_index < 0:\n                obj = obj.get(node_name)\n            else:\n                obj = obj.get(node_name)[array_index]\n            \n        val = obj    \n    except Exception as e:\n        pass\n    return val","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"def mergeDeltaWithExistingParquet(df, file_prefix, partition_cols, merge_keys, df_schema, s3obj):\n    \"\"\"\n    ::df: dataframe having delta records for the batch\n    ::file_prefix: The parquet file name. E.g: persona_inquiry_data\n    ::partition_cols: Columns used as partition keys. Must be a list, column names in the order of partition hierarchy. E.g: [\"part_appcreated_month\", \"part_appcreated_date\"]\n    ::merge_keys: Unique identifying columns that can be used to join delta dataframe with existing parquet data, inorder to identify update candidates\n    ::final_schema: Final list of columns to be stored into parquet, in the exact order.\n    \"\"\"\n    if df.empty:\n        print(\"mergeDeltaWithExistingParquet: Delta records dataset is empty. Nothing to merge.\")\n        return df\n    \n    df_final = pd.DataFrame()\n    records_written = 0\n    try:\n        parquet_exists = s3obj.existS3Key(file_prefix)\n        \n        if not parquet_exists:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Does not exist.\")\n            df_final = df\n        else:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Exists. Preparing delta\")\n            \n            delta_partitions_list = getUniquePartitionValues(df)\n            existing_data_df = fetchExistingPartitions(delta_partitions_list, file_prefix, df_schema)\n    \n            # Left join existing records(in S3) with the incoming delta records. Use the columns that uniquely identifies a record\n            if not existing_data_df.empty:\n                # get only the merge columns from the delta set - just to do a left join with existing data\n                df_delta_filter = df[merge_keys].copy(deep=True).reset_index(drop=True)\n                df_delta_filter[\"new_delta\"] = 1\n                \n                # Join with the incoming delta set and get the new_delta column =1, if the record exists in the incoming delta set\n                existing_marked_df = existing_data_df.merge(df_delta_filter, how=\"left\", on=merge_keys).copy(deep=True)\n    \n                # Filter out records from existing S3 records, which found a match in the incoming delta set.\n                df_existing_portion = existing_marked_df.query(\"~(new_delta==1)\").copy(deep=True)\n                df_existing_portion[\"new_delta\"] = 0\n\n                df[\"new_delta\"] = 1 # Just to match the list of columns\n\n                # Now concatenate the incoming delta set (full), with those existing rows from S3 which does not exist in the new delta set.\n                # This makes the full record set for the target.\n                df_final = pd.concat([df_existing_portion, df])[df_schema]\n            else:\n                df_final = df\n\n    except Exception as e:\n        print(f\"mergeDeltaWithExistingParquet: {str(e)}\")\n    return df_final","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"# Write the final dataframe to parquet\ndef write2Parquet(df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table):\n    res = {}\n    try:\n        # Write the prepared dataset back into S3. \"overwrite_partitions\" mode is being used while writing into an existing parquet file.\n        # This will ensure that, if there is a record that needs to go into an existing S3 partition, then that entire partition is overwritten.\n        # The step above makes sure that the dataset being overwritten contains all records that must be present in the target partition.\n        if not df.empty:\n            df_final = df[parquet_schema].copy() # Selecting columns in the correct order\n            print(\"write2Parquet: Writing parquet...\")\n            resp = s3obj.writeParquet(df_final, file_prefix, partition_cols, parquet_definition, glue_database, glue_table)\n            if not resp: \n                raise Exception(f\"Failed to write into [{file_prefix}] parquet.\")    \n            res = {\"status\":True, \"records_written\": df.shape[0]}\n        else:\n            res = {\"status\":True, \"records_written\": 0}\n            print(\"write2Parquet: Final delta record set is empty. Nothing to write to S3.\")\n    except Exception as e:\n        print(f\"write2Parquet: {str(e)}\")\n        res = {\"status\":False, \"records_written\": 0}\n    return res\n    \ndef map_parquet_types_to_pandas(merge_keys, parquet_definition):\n    type_map = None\n    try:\n        type_map = {}\n        for col in merge_keys:\n            parquet_type = parquet_definition.get(col)\n            pandas_type = \"string\"\n            if parquet_type in ['bigint', 'integer']:\n                pandas_type = \"Int64\"\n            if parquet_type in ['double']:\n                pandas_type = \"float64\"\n            if parquet_type in ['date']:\n                pandas_type = \"datetime64\"                \n            # Add more if needed\n            \n            type_map[col] = pandas_type\n    except Exception as e:\n        print(f\"map_parquet_types_to_pandas: {str(e)}\")\n    return type_map","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"def updateBatchControl(df, timestamp_column_name, batch_code, rdb_conn):\n    \"\"\"\n    df: Dataframe containing all the delta records that were written into parquet\n    timestamp_column_name: Column containing the timestamp value (preferably modified date time) in epoch format\n    batch_code: The batch code of the table archival script (PK from batch_control table)\n    rdb_conn: Redshift connection object\n    \"\"\"\n    if df.empty:\n        return True\n\n    try:\n        # Calculate the max(timestamp) from the given timestamp column in the dataframe.\n        max_timestamp = res[timestamp_column_name].max()\n        # Convert epoch number into formatted string representation\n        next_start_time = datetime.fromtimestamp(max_timestamp / 1000.0).strftime('%Y-%m-%d %H:%M:%S.%f')\n        # Prepare update SQL for batch control\n        updsql = \"update stage.batch_control set LAST_TIMESTAMP=%s where BATCH_CODE=%s\"\n        updparams = [next_start_time, batch_code]\n        updresult = rdb_conn.execute(updsql, updparams)\n\n        if updresult is None or updresult != 1:\n            raise Exception(\"Batch_control update failed\")\n    except Exception as e:\n        print(f\"updateBatchControl: {str(e)}\")\n        return False\n    return True","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"class RedshiftDb(object):\n    def __init__(self, config):\n        self.env = config.get(\"env\")\n        self.logger = logging.getLogger(\"RedshiftDb\")\n        self.dbConn = self._get_db_connection(config)\n\n    def _get_db_connection(self, config):\n        dbConn = None\n        try:\n            dbConn = redshift_connector.connect(host=config.get(\"REDSHIFT_HOST\"),\n                                                port=config.get(\"REDSHIFT_PORT\"),\n                                                database=config.get(\"REDSHIFT_DATABASE\"),\n                                                user=config.get(\"REDSHIFT_USER\"),\n                                                password=config.get(\"REDSHIFT_PWD\")\n                                                )\n            if not dbConn:\n                raise Exception(\"Could not connect\")\n        except Exception as e:\n            self.logger.error(f\"connect: {str(e)}\")\n        return dbConn\n\n\n    def fetchAll(self, sql, params=None):\n        \"\"\"\n        Takes a SQL string optional list of parameters as input\n        Returns pandas dataframe if the query was successful and has records.\n        On error or when result is empty, returns a None\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            cursor.execute(sql, params)\n            result = cursor.fetch_dataframe()\n\n        except Exception as e:\n            self.logger.error(f\"fetchall: {str(e)}\")\n        return result\n\n\n    def execute(self, sql, params=None):\n        \"\"\"\n        Executes one sql with an optional list of parameters\n        Returns number of records affected, when the SQL is successful. None in case of errors\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            res = cursor.execute(sql, params)\n            self.dbConn.commit()\n            result = res.rowcount\n\n        except Exception as e:\n            self.logger.error(f\"execute: {str(e)}\")\n            if self.dbConn:\n                self.dbConn.rollback()\n        return result","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"print(f\"Starting Batch Job: Incremental dump of persona table(s) data into Parquet\")\nconfig = getEnvConfig(env=\"DEV\")\nif not config:\n    print(f\"ERROR: Could not get credential configurations.\")\n    ","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"s3obj = AwsS3(config)\ns3obj.initialize()\n\n#rdb = RedshiftDb(config)  # <<<<<---- Uncomment once Sprinkle moves to neumoney n/w\nbatch_code = \"CRM_USER_FRESHWORKS_MAPPING_PARQUET\"\ntimestamp_column = \"modified_date\"\n\nglue_database = \"archive_redshift_db\"\nglue_table = \"crm_user_freshworks_mapping_details_data\"\n\nfile_prefix = \"application_data/crm/crm_user_freshworks_mapping_details_data_v0\"\npartition_cols = [\"part_created_month\", \"part_created_date\"]","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"# Read the delta records from crm_user_freshworks_mapping_detail_data tables, using the explore object. (Explore name: crm_user_freshworks_mapping_detail_data)\nexplore_id = '4a4f73caf8d54ccc9ade4e03a74daa17' #crm_user_freshworks_mapping_detail_data\ndf = sp.read_explore(explore_id)\n\nsql = (\" SELECT ucd.id, \"\n    \"ucd.user_uuid,\"\n    \" ucd.design_id,  \"\n    \"ucd.created_date,ucd.modified_date, ucd.is_deleted, ucd.applied_credit_limit, \n       \"TIMESTAMP 'epoch' + ucd.modified_date/1000 *INTERVAL '1 second' as modified_timestamp,\"\n       \"TIMESTAMP 'epoch' + ucd.created_date/1000 *INTERVAL '1 second' as created_timestamp,\"\n    \"current_timestamp as batch_timestamp  \"\n    \" from \"\n    \" FROM stage.ds_postgres_credit_card_user_card_design_redshift_user_card_design_stg ucd\"\n      \n    \" join public.ds_persona_inquiry_json_batch_persona_inquiry_json_batch bc on ucd.modified_date >= bc.last_timestamp and batch_code = %s \"\n    )\nsql_params = [batch_code]\n# Uncomment once Sprinkle has moved to neumoney network\n# Fetch data from stage table in redshift\n# df = rdb.fetchAll(sql, sql_params)\n\n\n# Convert the application created datetime into YYYYMMDD format for partitioning key\ndf[\"part_created_date\"] = df[\"created_timestamp\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m%d\"))  # If the timestamp is datetime\ndf[\"part_created_month\"] = df[\"created_timestamp\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m\"))  # If the timestamp is datetime\n\nprint(f\"Fetched {df.shape[0]} records to be added into [credit_card_user_card_design_detail_data] S3 file\")","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"# Columns used to identify unique records from the data set\nmerge_keys = ['id','user_uuid','design_id','part_created_month','part_created_date']\n\n# merge_keys_JSON[0] shauld be primary key and merge_keys_JSON[1] shauld be created date\nmerge_keys_JSON = ['user_uuid','created_timestamp','id','design_id','part_created_month','part_created_date']\n\nparquet_definition = {'id':'bigint','user_uuid':'string','design_id':'bigint','created_date':'bigint',\n                    'modified_date':'bigint','is_deleted':'string','applied_credit_limit':'string',\n                    'modified_timestamp':'date','created_timestamp':'date','json_data':'string'}\n\n\ndf_schema = list(parquet_definition.keys())\ndf_schema.extend(partition_cols)\n\n# Any calculated columns, process here\n# Bring these column values from the JSON log storage\n# None\nparquet_schema = list(parquet_definition.keys())\nparquet_schema.extend(partition_cols)\n\n# Convert column data types in the delta dataframe, inorder to match the data types of the existing parquet file\n# If the delta dataframe's data types for merge keys are not matching with parquet file's data types, while merging these two (inorder to remove duplicates), will not find matching records and will cause duplicates\npandas_type_map = map_parquet_types_to_pandas(merge_keys, parquet_definition)\nprint(pandas_type_map)\n\n# Convert pandas dataframe column types using the map\ndf_new = df_new.astype(pandas_type_map)\n","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"# Merge the delta records fetched from the explore object with existing parquet file in S3, so that every affected partition is reconstructed in the dataframe. \n# Note: Overlapping partitions will be overwritten in S3.\n\nfinal_df = mergeDeltaWithExistingParquet(df_new, file_prefix, partition_cols, merge_keys, df_schema, s3obj)\nprint(final_df.shape)\n\n# Now extract relevant attributes from various JSON fields and add them as columns into final_df.\n# For persona, there is one JSON column - verification_data\n#print(\"Extracting JSON attributes and adding to final dataframe...\")\n#final_df = flatten_verification_data(final_df)","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"# Write the data into parquet, with the parquet schema.\nresp = write2Parquet(final_df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table)\nprint(resp)","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"# Update batch control record for the batch job with the latest time stamp\nif resp.get(\"status\") and resp.get(\"records_written\") > 0:\n    #updateBatchControl(final_df, timestamp_column, batch_code, rdb)\n    print(f\"Updated batch_control: {batch_code}\")\nelse:\n    print(f\"No records written. batch_control not updated\")","cell_type":"code","execution_count":0}],"metadata":{"kernelspec":{"name":"python3","language":"","display_name":""},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":2}