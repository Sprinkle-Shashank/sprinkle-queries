{"cells":[{"metadata":{"trusted":true},"outputs":[],"source":" !pip install awswrangler","cell_type":"code","execution_count":1},{"metadata":{"trusted":true},"outputs":[],"source":"import pandas as pd\nfrom sprinkleSdk import SprinkleSdk as sp\nimport boto3\nimport awswrangler as wr\nfrom datetime import datetime\nimport logging\nimport json\nimport redshift_connector","cell_type":"code","execution_count":2},{"metadata":{},"outputs":[],"source":"def getBoto3Session(aws_access_key, aws_secret_access_key, region):\n    boto3session = None\n    try:\n        boto3session = boto3.Session(aws_access_key_id=aws_access_key,\n                                aws_secret_access_key=aws_secret_access_key,\n                                region_name=region)\n    except Exception as e:\n        print(f\"getBoto3Session: {str(e)}\")\n    return boto3session\n\ndef getEnvConfig(env=None):\n    # Specify the AWS user's access key/secret key here.\n    AWS_ACCESS_KEY_ID = \"AKIAVMM664F75NS4NJOL\"\n    AWS_SECRET_ACCESS_KEY = \"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\"\n    AWS_REGION_NAME = 'us-east-1'\n    try:\n\n        botoSession = getBoto3Session(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME)\n        if not botoSession:\n            raise Exception(\"Failed to obtain boto session\")\n\n        if not env or env == 'DEV':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": \"neumoney-dev-redshift-cluster.crwu8dyj5u4p.us-east-1.redshift.amazonaws.com\",\n                \"REDSHIFT_PORT\": 5439,\n                \"REDSHIFT_DATABASE\": \"neumoney-dev\",\n                \"REDSHIFT_USER\": \"neumoney\",\n                \"REDSHIFT_PWD\": \"p+6JVumb>g)QkiV\"\n            }\n        if env == 'PROD':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": wr.secretsmanager.get_secret(\"REDSHIFT_HOST\", botoSession),\n                \"REDSHIFT_PORT\": wr.secretsmanager.get_secret(\"REDSHIFT_PORT\", botoSession),\n                \"REDSHIFT_DATABASE\": wr.secretsmanager.get_secret(\"REDSHIFT_DATABASE\", botoSession),\n                \"REDSHIFT_USER\": wr.secretsmanager.get_secret(\"REDSHIFT_USER\", botoSession),\n                \"REDSHIFT_PWD\": wr.secretsmanager.get_secret(\"REDSHIFT_PWD\", botoSession)\n            }\n\n    except Exception as e:\n        print(f\"getConfig: {str(e)}\")\n        return None\n    return config","cell_type":"markdown","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"class AwsS3():\n    def __init__(self, config):\n        self.AWS_ACCESS_KEY_ID = config.get(\"AWS_ACCESS_KEY_ID\")\n        self.AWS_SECRET_ACCESS_KEY = config.get(\"AWS_SECRET_ACCESS_KEY\")\n        self.region_name = config.get(\"AWS_REGION_NAME\")\n        self.bucket_name = config.get(\"BUCKET_NAME\") \n        self.boto3session = None\n        self.s3client = None\n\n    def initialize(self):\n        self.getBoto3Session()\n        self.getBoto3S3Client()\n        return True\n    \n    def getS3Url(self, file_prefix):\n        url = f\"s3://{self.bucket_name}/{file_prefix}\"\n        return url\n    \n    def getBoto3Session(self):\n        try:\n            session = boto3.Session(aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                                    aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n                                    region_name=self.region_name)\n            self.boto3session = session\n        except Exception as e:\n            print(f\"getBoto3Session: {str(e)}\")\n            return False\n        return True\n\n    \n    def getBoto3S3Client(self):\n        try:\n            s3_client = boto3.client('s3', aws_access_key_id=self.AWS_ACCESS_KEY_ID, aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,region_name=self.region_name)\n            self.s3client = s3_client\n        except Exception as e:\n            print(f\"getBoto3S3Client: {str(e)}\")\n            return False\n        return True        \n        \n    def existS3Key(self, prefix):\n        found = False\n        try:\n            result = self.s3client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)\n            if 'Contents' in result.keys():\n                found = True\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return found      \n    \n    def writeParquet(self, df, file_prefix, partition_cols, parquet_definition, glue_database, glue_table):\n        \"\"\"\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html\n        file_prefix: the prefix for the table. E.g: \"s3://neu-data-analytics/application_data/application_data_identity/user_data_v0\"\n        partition_cols: List of columns by which the parquet is to be partitioned\n        \"\"\"\n        resp = {}\n        try:\n            if df.empty:\n                raise Exception(\"Dataframe empty. Nothing to write to S3.\")\n                \n            partitioned =True if partition_cols else False\n            s3url = self.getS3Url(file_prefix)\n            file_exists = self.existS3Key(file_prefix)\n\n            if file_exists:\n                #logging.info(f\"URL:{s3url} exists. Overwriting partitions...\")\n                print(logging.info(f\"URL:{s3url} exists. Overwriting partitions...\"))\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        mode='overwrite_partitions',\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition\n                                       )\n            else:\n                logging.info(f\"URL:{s3url} does not exist. Creating...\")\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition)\n            \n            if not resp:\n                raise Exception (f'Failed to write parquet at: {s3url}')\n            \n        except Exception as e:\n            print(f\"writeParquet: {str(e)}\")\n            return resp\n        return resp          \n    \n    def readParquet(self, file_prefix, partition_predicate):\n        \"\"\"\n        Reads entire parquet dataset if partition_predicate is None, If supplied, reads one partition. Returns a pandas dataframe.\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.read_parquet.html\n        ::file_prefix: Prefix for the table. E.g: \"s3://neu-data-analytics/persona_data/inquiry_data\" \n        ::partition_predicate: partition string: E.g: \"<column_name1>=<value>/<column_name2>=<value>/....\" format. Depends on the number of levels of partitioning\n        \"\"\"\n        df = pd.DataFrame()\n        urls = []\n        try:\n            # Read each partition from the supplied list. \n            s3url = self.getS3Url(file_prefix)\n            \n            # If there are partitions specified, read only those specific partitions. Else read the entire parquet file\n            if partition_predicate:\n                url = f\"{s3url}/{partition_predicate}\"\n            else:\n                url = s3url\n            \n            print(url)    \n            df = wr.s3.read_parquet(path=url, dataset=True, boto3_session=self.boto3session, ignore_empty=True, ignore_index=True, path_suffix=[\".snappy.parquet\"])   \n            #print(\"DF type is this \",type(df),)  \n        except Exception as e:\n            print(f\"readParquet: {str(e)}\")\n        return df ","cell_type":"code","execution_count":4},{"metadata":{"trusted":true},"outputs":[],"source":"def getUniquePartitionValues(df):\n    return df[[\"part_appcreated_month\",\"part_appcreated_date\"]].drop_duplicates().to_dict('records')","cell_type":"code","execution_count":5},{"metadata":{"trusted":true},"outputs":[],"source":"def fetchExistingPartitions(parts_list, file_prefix, df_schema):\n    \"\"\"\n    Given a list of partition predicates, read all those partitions and concatenate them into a single pandas dataframe\n    Each item in parts_list will be of the type: {\"part_appcreated_month\": \"202210\", \"part_appcreated_date\": \"20221005\"}\n    \"\"\"\n    part_dfs = []\n    existing_data_df = pd.DataFrame()\n    try:\n        for partition_predicate in parts_list:\n            df_part = s3obj.readParquet(file_prefix, f\"part_appcreated_month={partition_predicate.get('part_appcreated_month')}/part_appcreated_date={partition_predicate.get('part_appcreated_date')}\")\n            if not df_part.empty:\n                df_part[\"part_appcreated_month\"] = partition_predicate.get('part_appcreated_month')\n                df_part[\"part_appcreated_date\"] = partition_predicate.get('part_appcreated_date')                \n                part_dfs.append(df_part)\n            \n        if part_dfs:\n            existing_data_df = pd.concat(part_dfs)[df_schema] # Return only the columns which will be present in the incoming delta dataset. Strip all derived columns existing in parquet \n    except Exception as e:\n        print(f\"fetchExistingPartitions: {str(e)}\")\n    return existing_data_df","cell_type":"code","execution_count":6},{"metadata":{"trusted":true},"outputs":[],"source":"def mergeDeltaWithExistingParquet(df, file_prefix, partition_cols, merge_keys, df_schema, s3obj):\n    \"\"\"\n    ::df: dataframe having delta records for the batch\n    ::file_prefix: The parquet file name. E.g: persona_inquiry_data\n    ::partition_cols: Columns used as partition keys. Must be a list, column names in the order of partition hierarchy. E.g: [\"part_appcreated_month\", \"part_appcreated_date\"]\n    ::merge_keys: Unique identifying columns that can be used to join delta dataframe with existing parquet data, inorder to identify update candidates\n    ::final_schema: Final list of columns to be stored into parquet, in the exact order.\n    \"\"\"\n    if df.empty:\n        print(\"mergeDeltaWithExistingParquet: Delta records dataset is empty. Nothing to merge.\")\n        return df\n    \n    \n    df_final = pd.DataFrame()\n    records_written = 0\n    try:\n        parquet_exists = s3obj.existS3Key(file_prefix)\n        \n        if not parquet_exists:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Does not exist.\")\n            df_final = df\n        else:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Exists. Preparing delta\")\n            \n            delta_partitions_list = getUniquePartitionValues(df)\n            existing_data_df = fetchExistingPartitions(delta_partitions_list, file_prefix, df_schema)\n    \n            # Left join existing records(in S3) with the incoming delta records. Use the columns that uniquely identifies a record\n            if not existing_data_df.empty:\n                # get only the merge columns from the delta set - just to do a left join with existing data\n                df_delta_filter = df[merge_keys].copy(deep=True).reset_index(drop=True)\n                df_delta_filter[\"new_delta\"] = 1\n\n                # Join with the incoming delta set and get the new_delta column =1, if the record exists in the incoming delta set\n                existing_marked_df = existing_data_df.merge(df_delta_filter, how=\"left\", on=merge_keys).copy(deep=True)\n    \n                # Filter out records from existing S3 records, which found a match in the incoming delta set.\n                df_existing_portion = existing_marked_df.query(\"~(new_delta==1)\").copy(deep=True)\n                df_existing_portion[\"new_delta\"] = 0\n\n                df[\"new_delta\"] = 1 # Just to match the list of columns\n\n                # Now concatenate the incoming delta set (full), with those existing rows from S3 which does not exist in the new delta set.\n                # This makes the full record set for the target.\n                df_final = pd.concat([df_existing_portion, df])[df_schema]\n            else:\n                df_final = df\n\n    except Exception as e:\n        print(f\"mergeDeltaWithExistingParquet: {str(e)}\")\n    return df_final","cell_type":"code","execution_count":7},{"metadata":{"trusted":true},"outputs":[],"source":"# Write the final dataframe to parquet\ndef write2Parquet(df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table):\n    res = {}\n    try:\n        # Write the prepared dataset back into S3. \"overwrite_partitions\" mode is being used while writing into an existing parquet file.\n        # This will ensure that, if there is a record that needs to go into an existing S3 partition, then that entire partition is overwritten.\n        # The step above makes sure that the dataset being overwritten contains all records that must be present in the target partition.\n        if not df.empty:\n            df_final = df[parquet_schema].copy() # Selecting columns in the correct order\n            print(\"write2Parquet: Writing parquet...\")\n            resp = s3obj.writeParquet(df_final, file_prefix, partition_cols, parquet_definition, glue_database, glue_table)\n            if not resp: \n                raise Exception(f\"Failed to write into [{file_prefix}] parquet.\")    \n            res = {\"status\":True, \"records_written\": df.shape[0]}\n        else:\n            res = {\"status\":True, \"records_written\": 0}\n            print(\"write2Parquet: Final delta record set is empty. Nothing to write to S3.\")\n    except Exception as e:\n        print(f\"write2Parquet: {str(e)}\")\n        res = {\"status\":False, \"records_written\": 0}\n    return res\n\ndef map_parquet_types_to_pandas(merge_keys, parquet_definition):\n    type_map = None\n    try:\n        type_map = {}\n        for col in merge_keys:\n            parquet_type = parquet_definition.get(col)\n            pandas_type = \"string\"\n            if parquet_type in ['bigint', 'integer']:\n                pandas_type = \"Int64\"\n            if parquet_type in ['double']:\n                pandas_type = \"float64\"\n            if parquet_type in ['date']:\n                pandas_type = \"datetime64\"                \n            # Add more if needed\n            \n            type_map[col] = pandas_type\n    except Exception as e:\n        print(f\"map_parquet_types_to_pandas: {str(e)}\")\n    return type_map","cell_type":"code","execution_count":8},{"metadata":{"trusted":true},"outputs":[],"source":"def updateBatchControl(df, timestamp_column_name, batch_code, rdb_conn):\n    \"\"\"\n    df: Dataframe containing all the delta records that were written into parquet\n    timestamp_column_name: Column containing the timestamp value (preferably modified date time) in epoch format\n    batch_code: The batch code of the table archival script (PK from batch_control table)\n    rdb_conn: Redshift connection object\n    \"\"\"\n    if df.empty:\n        return True\n\n    try:\n        # Calculate the max(timestamp) from the given timestamp column in the dataframe.\n        max_timestamp = res[timestamp_column_name].max()\n        # Convert epoch number into formatted string representation\n        next_start_time = datetime.fromtimestamp(max_timestamp / 1000.0).strftime('%Y-%m-%d %H:%M:%S.%f')\n        # Prepare update SQL for batch control\n        updsql = \"update stage.batch_control set LAST_TIMESTAMP=%s where BATCH_CODE=%s\"\n        updparams = [next_start_time, batch_code]\n        updresult = rdb_conn.execute(updsql, updparams)\n\n        if updresult is None or updresult != 1:\n            raise Exception(\"Batch_control update failed\")\n    except Exception as e:\n        print(f\"updateBatchControl: {str(e)}\")\n        return False\n    return True","cell_type":"code","execution_count":9},{"metadata":{"trusted":true},"outputs":[],"source":"class RedshiftDb(object):\n    def __init__(self, config):\n        self.env = config.get(\"env\")\n        self.logger = logging.getLogger(\"RedshiftDb\")\n        self.dbConn = self._get_db_connection(config)\n\n    def _get_db_connection(self, config):\n        dbConn = None\n        try:\n            dbConn = redshift_connector.connect(host=config.get(\"REDSHIFT_HOST\"),\n                                                port=config.get(\"REDSHIFT_PORT\"),\n                                                database=config.get(\"REDSHIFT_DATABASE\"),\n                                                user=config.get(\"REDSHIFT_USER\"),\n                                                password=config.get(\"REDSHIFT_PWD\")\n                                                )\n            if not dbConn:\n                raise Exception(\"Could not connect\")\n        except Exception as e:\n            self.logger.error(f\"connect: {str(e)}\")\n        return dbConn\n\n\n    def fetchAll(self, sql, params=None):\n        \"\"\"\n        Takes a SQL string optional list of parameters as input\n        Returns pandas dataframe if the query was successful and has records.\n        On error or when result is empty, returns a None\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            cursor.execute(sql, params)\n            result = cursor.fetch_dataframe()\n\n        except Exception as e:\n            self.logger.error(f\"fetchall: {str(e)}\")\n        return result\n\n\n    def execute(self, sql, params=None):\n        \"\"\"\n        Executes one sql with an optional list of parameters\n        Returns number of records affected, when the SQL is successful. None in case of errors\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            res = cursor.execute(sql, params)\n            self.dbConn.commit()\n            result = res.rowcount\n\n        except Exception as e:\n            self.logger.error(f\"execute: {str(e)}\")\n            if self.dbConn:\n                self.dbConn.rollback()\n        return result","cell_type":"code","execution_count":10},{"metadata":{"trusted":true},"outputs":[],"source":"#Connection\nprint(f\"Starting Batch Job: Incremental dump of CROSSRIVER table data into Parquet\")\nconfig = getEnvConfig(env=\"DEV\") # Change env=\"PROD\" while deploying to production\nif not config:\n    print(f\"ERROR: Could not get credential configurations.\")\n    ","cell_type":"code","execution_count":11},{"metadata":{"trusted":true},"outputs":[],"source":"s3obj = AwsS3(config)\ns3obj.initialize()\n\n#rdb = RedshiftDb(config)  # <<<<<---- Uncomment once Sprinkle moves to neumoney n/w\nbatch_code = \"I2C_JSON\"\ntimestamp_column = \"modified_date\"\n\nglue_database = \"archive_redshift_db\"\nglue_table = \"onboarding_i2c_bank_account_detail\"\n\nfile_prefix = \"application_data/onboarding/i2c_bank_account_detail_v0\"\npartition_cols = [\"part_appcreated_month\", \"part_appcreated_date\"]","cell_type":"code","execution_count":12},{"metadata":{"trusted":true},"outputs":[],"source":"# Read the delta records from scienaptic tables, using the explore object. (Explore name: i2c_detail_data)\nexplore_id ='1910be5e950a4726a271f29a4527ad31' #i2c_detail_data\ndf = sp.read_explore(explore_id)\n\n\nsql = (\"select tab.* from ( \"\n    \"select  \"\n    \"stg.id as i2c_id,\"\n    \"stg.user_uuid,\"\n    \"stg.bank_account_detail_uuid,\"\n    \"stg.application_id,\"\n    \"stg.account_sr_no,\"\n    \"stg.credit_card_uuid,\"\n    \"stg.card_reference_id,\"\n    \"stg.bank_name,\"\n    \"stg.routing_number,\"\n    \"stg.default_flag,\"\n    \"stg.response_code,\"\n    \"stg.response_desc,\"\n    \"stg.created_date,\"\n    \"stg.modified_date,\"\n    \"stg.account_number,\"\n    \"stg.is_deleted,\"\n    \"stg.account_id,\"\n    \"acc_typ.name as account_type,\"\n    \"ach_typ.name as ach_type,\"\n    \"acc_st.name as account_status,\"\n    \"acc_det_st.name as account_detail_status,\"\n    \"TIMESTAMP 'epoch' + stg.modified_date/1000 *INTERVAL '1 second' as modified_timestamp, \"\n    \"TIMESTAMP 'epoch' + apps.created_date/1000 *INTERVAL '1 second' as application_created_date,\"\n    \"current_timestamp as batch_timestamp  \"\n    \" from stage.ds_postgres_onboarding_i2c_bank_account_details_redshift_i2c_bank_account_details_stg stg \"\n    \" left join stage.ds_postgres_onboarding_application_redshift_application_stg apps on\"\n    \" stg.application_id=apps.id\"\n    \" left join stage.ds_postgres_onboarding_mst_i2c_bank_account_type_redshift_mst_i2c_bank_account_type_stg acc_typ\"\n    \" on stg.account_type=acc_typ.code\"\n    \" left join stage.ds_postgres_onboarding_mst_i2c_ach_type_redshift_mst_i2c_ach_type_stg ach_typ\"\n    \" on stg.ach_type=ach_typ.code\"\n    \" left join stage.ds_postgres_onboarding_mst_i2c_account_status_redshift_mst_i2c_account_status_stg acc_st\"\n    \" on stg.account_status=acc_st.code\"\n    \" left join stage.ds_postgres_onboarding_mst_i2c_account_details_status_redshift_mst_i2c_account_details_status_stg acc_det_st\"\n    \" on stg.account_detail_status=acc_det_st.code\"\n    \" ) tab\"\n    \" join stage.batch_control bc on tab.modified_timestamp >= bc.last_timestamp and batch_code = %s\"\n    )\nsql_params = [batch_code]\n# Uncomment once Sprinkle has moved to neumoney network\n# Fetch data from stage table in redshift\n# df = rdb.fetchAll(sql, sql_params)\n\n\n# Convert the application created datetime into YYYYMMDD format for partitioning key\ndf[\"part_appcreated_month\"] = df[\"application_created_date\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m\"))  \ndf[\"part_appcreated_date\"] = df[\"application_created_date\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m%d\"))  \n\n#print(f\"Fetched {df.shape[0]} records to be added into [{file_prefix}] S3 file\")","cell_type":"code","execution_count":13},{"metadata":{"trusted":true},"outputs":[],"source":"# Columns used to identify unique records from the data set\nmerge_keys = ['i2c_id','user_uuid','bank_account_detail_uuid','application_id']\n\nparquet_definition = {'i2c_id':'bigint','user_uuid':'string','bank_account_detail_uuid':'string',\n                'application_id':'bigint','account_sr_no':'string','credit_card_uuid':'string','card_reference_id':'string',\n                'bank_name':'string','routing_number':'string','default_flag':'boolean','response_code':'string',\n                'response_desc':'string','created_date':'bigint','modified_date':'bigint','account_number':'string',\n                'is_deleted':'boolean','account_id':'string','account_type':'string','ach_type':'string','account_status':'string',\n                'account_detail_status':'string','modified_timestamp':'date','application_created_date':'date','batch_timestamp':'date'}\n\ndf_schema = list(parquet_definition.keys())\ndf_schema.extend(partition_cols)\n\n# Any calculated columns, process here\n# Bring these column values from the JSON log storage\n# None\nparquet_schema = list(parquet_definition.keys())\nparquet_schema.extend(partition_cols)\n\n# Convert column data types in the delta dataframe, inorder to match the data types of the existing parquet file\n# If the delta dataframe's data types for merge keys are not matching with parquet file's data types, while merging these two (inorder to remove duplicates), will not find matching records and will cause duplicates\npandas_type_map = map_parquet_types_to_pandas(merge_keys, parquet_definition)\nprint(pandas_type_map)\n\n# Convert pandas dataframe column types using the map\ndf = df.astype(pandas_type_map)","cell_type":"code","execution_count":14},{"metadata":{"trusted":true},"outputs":[],"source":"# Merge the delta records fetched from the explore object with existing parquet file in S3, so that every affected partition is reconstructed in the dataframe. \n# Note: Overlapping partitions will be overwritten in S3.\n\nfinal_df = mergeDeltaWithExistingParquet(df, file_prefix, partition_cols, merge_keys, df_schema, s3obj)\nprint(final_df.shape)\n\n# # Now extract relevant attributes from various JSON fields and add them as columns into final_df.\n# # For cross_river, there is no jsi\n# print(\"Extracting JSON attributes and adding to final dataframe...\")\n# final_df = df","cell_type":"code","execution_count":15},{"metadata":{"trusted":true},"outputs":[],"source":"#  Write the data into parquet, with the parquet schema.\nresp = write2Parquet(final_df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table)\nprint(resp)","cell_type":"code","execution_count":16},{"metadata":{"trusted":true},"outputs":[],"source":"# Update batch control record for the batch job with the latest time stamp\nif resp.get(\"status\") and resp.get(\"records_written\") > 0:\n    #updateBatchControl(final_df, timestamp_column, batch_code, rdb)\n    print(f\"Updated batch_control: {batch_code}\")\nelse:\n    print(f\"No records written. batch_control not updated\")","cell_type":"code","execution_count":17},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.10.9","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":2}