{"cells":[{"metadata":{"trusted":true},"outputs":[],"source":"!pip install awswrangler","cell_type":"code","execution_count":1},{"metadata":{"trusted":true},"outputs":[],"source":"import pandas as pd\nfrom sprinkleSdk import SprinkleSdk as sp\nimport boto3\nimport awswrangler as wr\nfrom datetime import datetime\nimport logging\nimport json\nimport redshift_connector","cell_type":"code","execution_count":2},{"metadata":{"trusted":true},"outputs":[],"source":"def getBoto3Session(aws_access_key, aws_secret_access_key, region):\n    boto3session = None\n    try:\n        boto3session = boto3.Session(aws_access_key_id=aws_access_key,\n                                aws_secret_access_key=aws_secret_access_key,\n                                region_name=region)\n    except Exception as e:\n        print(f\"getBoto3Session: {str(e)}\")\n    return boto3session\n\ndef getEnvConfig(env=None):\n    # Specify the AWS user's access key/secret key here.\n    AWS_ACCESS_KEY_ID = \"AKIAVMM664F73YXKHU7A\"\n    AWS_SECRET_ACCESS_KEY = \"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\"\n    AWS_REGION_NAME = 'us-east-1'\n    try:\n\n        botoSession = getBoto3Session(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME)\n        if not botoSession:\n            raise Exception(\"Failed to obtain boto session\")\n\n        if not env or env == 'DEV':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": \"neumoney-dev-redshift-cluster.crwu8dyj5u4p.us-east-1.redshift.amazonaws.com\",\n                \"REDSHIFT_PORT\": 5439,\n                \"REDSHIFT_DATABASE\": \"neumoney-dev\",\n                \"REDSHIFT_USER\": \"neumoney\",\n                \"REDSHIFT_PWD\": \"p+6JVumb>g)QkiV\"\n            }\n        if env == 'PROD':\n            config = {\n                \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n                \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n                \"AWS_REGION_NAME\": AWS_REGION_NAME,\n                \"BUCKET_NAME\": \"neu-data-analytics\",\n                \"REDSHIFT_HOST\": wr.secretsmanager.get_secret(\"REDSHIFT_HOST\", botoSession),\n                \"REDSHIFT_PORT\": wr.secretsmanager.get_secret(\"REDSHIFT_PORT\", botoSession),\n                \"REDSHIFT_DATABASE\": wr.secretsmanager.get_secret(\"REDSHIFT_DATABASE\", botoSession),\n                \"REDSHIFT_USER\": wr.secretsmanager.get_secret(\"REDSHIFT_USER\", botoSession),\n                \"REDSHIFT_PWD\": wr.secretsmanager.get_secret(\"REDSHIFT_PWD\", botoSession)\n            }\n\n    except Exception as e:\n        print(f\"getConfig: {str(e)}\")\n        return None\n    return config","cell_type":"code","execution_count":3},{"metadata":{"trusted":true},"outputs":[],"source":"class AwsS3():\n    def __init__(self, config):\n        self.AWS_ACCESS_KEY_ID = config.get(\"AWS_ACCESS_KEY_ID\")\n        self.AWS_SECRET_ACCESS_KEY = config.get(\"AWS_SECRET_ACCESS_KEY\")\n        self.region_name = config.get(\"AWS_REGION_NAME\")\n        self.bucket_name = config.get(\"BUCKET_NAME\") \n        self.boto3session = None\n        self.s3client = None\n\n    def initialize(self):\n        self.getBoto3Session()\n        self.getBoto3S3Client()\n        return True\n    \n    def getS3Url(self, file_prefix):\n        url = f\"s3://{self.bucket_name}/{file_prefix}\"\n        return url\n    \n    def getBoto3Session(self):\n        try:\n            session = boto3.Session(aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                                    aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n                                    region_name=self.region_name)\n            self.boto3session = session\n        except Exception as e:\n            print(f\"getBoto3Session: {str(e)}\")\n            return False\n        return True\n\n    \n    def getBoto3S3Client(self):\n        try:\n            s3_client = boto3.client('s3', aws_access_key_id=self.AWS_ACCESS_KEY_ID, aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,region_name=self.region_name)\n            self.s3client = s3_client\n        except Exception as e:\n            print(f\"getBoto3S3Client: {str(e)}\")\n            return False\n        return True        \n        \n    def existS3Key(self, prefix):\n        found = False\n        try:\n            result = self.s3client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)\n            if 'Contents' in result.keys():\n                found = True\n        except Exception as e:\n            print(f\"existS3Key: {str(e)}\")\n            return False\n        return found      \n    \n    def writeParquet(self, df, file_prefix, partition_cols, parquet_definition, glue_database, glue_table):\n        \"\"\"\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html\n        file_prefix: the prefix for the table. E.g: \"s3://neu-data-analytics/application_data/application_data_identity/user_data_v0\"\n        partition_cols: List of columns by which the parquet is to be partitioned\n        \"\"\"\n        resp = {}\n        try:\n            if df.empty:\n                raise Exception(\"Dataframe empty. Nothing to write to S3.\")\n                \n            partitioned =True if partition_cols else False\n            s3url = self.getS3Url(file_prefix)\n            file_exists = self.existS3Key(file_prefix)\n\n            if file_exists:\n                #logging.info(f\"URL:{s3url} exists. Overwriting partitions...\")\n                print(logging.info(f\"URL:{s3url} exists. Overwriting partitions...\"))\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        mode='overwrite_partitions',\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition\n                                       )\n            else:\n                logging.info(f\"URL:{s3url} does not exist. Creating...\")\n                resp = wr.s3.to_parquet(df=df,\n                                        path=s3url,\n                                        boto3_session=self.boto3session,\n                                        dataset=True,\n                                        partition_cols=partition_cols,\n                                        database=glue_database,\n                                        table=glue_table,\n                                        dtype=parquet_definition)\n            \n            if not resp:\n                raise Exception (f'Failed to write parquet at: {s3url}')\n            \n        except Exception as e:\n            print(f\"writeParquet: {str(e)}\")\n            return resp\n        return resp           \n    \n    def readParquet(self, file_prefix, partition_predicate):\n        \"\"\"\n        Reads entire parquet dataset if partition_predicate is None, If supplied, reads one partition. Returns a pandas dataframe.\n        Reference: https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.read_parquet.html\n        ::file_prefix: Prefix for the table. E.g: \"s3://neu-data-analytics/application_data/credit_card/i2c_transaction_history_live_data_v0\"\n        ::partition_predicate: partition string: E.g: \"<column_name1>=<value>/<column_name2>=<value>/....\" format. Depends on the number of levels of partitioning\n        \"\"\"\n        df = pd.DataFrame()\n        urls = []\n        try:\n            # Read each partition from the supplied list. \n            s3url = self.getS3Url(file_prefix)\n            \n            # If there are partitions specified, read only those specific partitions. Else read the entire parquet file\n            if partition_predicate:\n                url = f\"{s3url}/{partition_predicate}\"\n            else:\n                url = s3url\n            \n            print(url)    \n            df = wr.s3.read_parquet(path=url, dataset=True, boto3_session=self.boto3session, ignore_empty=True, ignore_index=True, path_suffix=[\".snappy.parquet\"])   \n            #print(\"DF type is this \",type(df),)  \n        except Exception as e:\n            print(f\"readParquet: {str(e)}\")\n        return df \n    def readJSON(self, file_prefix):\n        \"\"\"\n        Reads a JSON file with the specified file_prefix from the S3 bucket. \n        Returns a dictionary of the format {\"data\": {..json data read from s3 file}, \"status\": \"ok|FileNotFound|error|...\"}\n        \"\"\"\n        resp = {\"data\":None, \"status\":None}\n        try:\n            if self.existS3Key(file_prefix):\n                result = self.s3client.get_object(Bucket=self.bucket_name, Key=file_prefix)\n                if not result:\n                    raise Exception(\"ReadError\")\n                jsonbody = result.get(\"Body\").read().decode()\n                if not jsonbody:\n                    raise Exception(\"EmptyJson\")\n                resp = {\"data\":jsonbody, \"status\":\"ok\"}\n            else:\n                raise Exception(\"FileNotFound\")\n        except Exception as e:\n            resp = {\"status\":str(e), \"data\":None}\n            print(f\"readJSON: {str(e)}\")\n        return resp","cell_type":"code","execution_count":4},{"metadata":{"trusted":true},"outputs":[],"source":"def getUniquePartitionValues(df):\n    return df[[\"part_created_month\",\"part_created_date\"]].drop_duplicates().to_dict('records')","cell_type":"code","execution_count":5},{"metadata":{"trusted":true},"outputs":[],"source":"def getJsonData(dataframe,merge_keys_json,json_col_name,json_file_path,table_name):\n    try:\n        json_list = []\n        for index, row in df.iterrows():\n            row_json = dict()        \n            file_prefix_json = '{}/{}_{}_{}.json'.format(json_file_path,table_name,row[merge_keys_json[0]],row[merge_keys_json[1]].replace(':',''))\n#             print(file_prefix_json)\n            json_data = s3obj.readJSON(file_prefix_json)\n            if json_data['status'] == 'ok': \n                stud_obj = json.loads(json_data['data'])\n                row_json[json_col_name] = str(stud_obj)\n            else:\n                row_json[json_col_name] = str()\n            for ln in merge_keys_json:\n                row_json[ln] = row[ln]\n\n            json_list.append(row_json)\n    except Exception as e:\n        print(f\"getJSONData: {str(e)}\")\n    return json_list","cell_type":"code","execution_count":6},{"metadata":{"trusted":true},"outputs":[],"source":"def fetchExistingPartitions(parts_list, file_prefix, df_schema):\n    \"\"\"\n    Given a list of partition predicates, read all those partitions and concatenate them into a single pandas dataframe\n    Each item in parts_list will be of the type: {\"part_created_month\": \"202210\", \"part_created_date\": \"20221005\"}\n    \"\"\"\n    part_dfs = []\n    existing_data_df = pd.DataFrame()\n    try:\n        for partition_predicate in parts_list:\n            df_part = s3obj.readParquet(file_prefix, f\"part_created_month={partition_predicate.get('part_created_month')}/part_created_date={partition_predicate.get('part_created_date')}\")\n            if not df_part.empty:\n                df_part[\"part_created_month\"] = partition_predicate.get('part_created_month')\n                df_part[\"part_created_date\"] = partition_predicate.get('part_created_date')\n                part_dfs.append(df_part)\n        if part_dfs:\n            existing_data_df = pd.concat(part_dfs)[df_schema] # Return only the columns which will be present in the incoming delta dataset. Strip all derived columns existing in parquet \n    except Exception as e:\n        print(f\"fetchExistingPartitions: {str(e)}\")\n    return existing_data_df","cell_type":"code","execution_count":7},{"metadata":{"trusted":true},"outputs":[],"source":"def mergeDeltaWithExistingParquet(df, file_prefix, partition_cols, merge_keys, df_schema, s3obj):\n    \"\"\"\n    ::df: dataframe having delta records for the batch\n    ::file_prefix: The parquet file name. E.g: i2c_transaction_history_live_data_v0\n    ::partition_cols: Columns used as partition keys. Must be a list, column names in the order of partition hierarchy. E.g: [\"part_appcreated_month\", \"part_appcreated_date\"]\n    ::merge_keys: Unique identifying columns that can be used to join delta dataframe with existing parquet data, inorder to identify update candidates\n    ::final_schema: Final list of columns to be stored into parquet, in the exact order.\n    \"\"\"\n    if df.empty:\n        print(\"mergeDeltaWithExistingParquet: Delta records dataset is empty. Nothing to merge.\")\n        return df\n    \n    \n    df_final = pd.DataFrame()\n    records_written = 0\n    try:\n        parquet_exists = s3obj.existS3Key(file_prefix)\n        \n        if not parquet_exists:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Does not exist.\")\n            df_final = df\n        else:\n            print(f\"mergeDeltaWithExistingParquet: Parquet file {file_prefix} Exists. Preparing delta\")\n            \n            delta_partitions_list = getUniquePartitionValues(df)\n            existing_data_df = fetchExistingPartitions(delta_partitions_list, file_prefix, df_schema)\n    \n            # Left join existing records(in S3) with the incoming delta records. Use the columns that uniquely identifies a record\n            if not existing_data_df.empty:\n                # get only the merge columns from the delta set - just to do a left join with existing data\n                df_delta_filter = df[merge_keys].copy(deep=True).reset_index(drop=True)\n                df_delta_filter[\"new_delta\"] = 1\n\n                # Join with the incoming delta set and get the new_delta column =1, if the record exists in the incoming delta set\n                existing_marked_df = existing_data_df.merge(df_delta_filter, how=\"left\", on=merge_keys).copy(deep=True)\n    \n                # Filter out records from existing S3 records, which found a match in the incoming delta set.\n                df_existing_portion = existing_marked_df.query(\"~(new_delta==1)\").copy(deep=True)\n                df_existing_portion[\"new_delta\"] = 0\n\n                df[\"new_delta\"] = 1 # Just to match the list of columns\n\n                # Now concatenate the incoming delta set (full), with those existing rows from S3 which does not exist in the new delta set.\n                # This makes the full record set for the target.\n                df_final = pd.concat([df_existing_portion, df])[df_schema]\n            else:\n                df_final = df\n\n    except Exception as e:\n        print(f\"mergeDeltaWithExistingParquet: {str(e)}\")\n    return df_final","cell_type":"code","execution_count":8},{"metadata":{"trusted":true},"outputs":[],"source":"# Write the final dataframe to parquet\ndef write2Parquet(df, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table):\n    res = {}\n    try:\n        # Write the prepared dataset back into S3. \"overwrite_partitions\" mode is being used while writing into an existing parquet file.\n        # This will ensure that, if there is a record that needs to go into an existing S3 partition, then that entire partition is overwritten.\n        # The step above makes sure that the dataset being overwritten contains all records that must be present in the target partition.\n        if not df.empty:\n            df_final = df[parquet_schema].copy() # Selecting columns in the correct order\n            print(\"write2Parquet: Writing parquet...\")\n            resp = s3obj.writeParquet(df_final, file_prefix, partition_cols, parquet_definition, glue_database, glue_table)\n            if not resp: \n                raise Exception(f\"Failed to write into [{file_prefix}] parquet.\")    \n            res = {\"status\":True, \"records_written\": df.shape[0]}\n        else:\n            res = {\"status\":True, \"records_written\": 0}\n            print(\"write2Parquet: Final delta record set is empty. Nothing to write to S3.\")\n    except Exception as e:\n        print(f\"write2Parquet: {str(e)}\")\n        res = {\"status\":False, \"records_written\": 0}\n    return res\n    \ndef map_parquet_types_to_pandas(merge_keys, parquet_definition):\n    type_map = None\n    try:\n        type_map = {}\n        for col in merge_keys:\n            parquet_type = parquet_definition.get(col)\n            pandas_type = \"string\"\n            if parquet_type in ['bigint', 'integer']:\n                pandas_type = \"Int64\"\n            if parquet_type in ['double']:\n                pandas_type = \"float64\"\n            if parquet_type in ['date']:\n                pandas_type = \"datetime64\"                \n            # Add more if needed\n            \n            type_map[col] = pandas_type\n    except Exception as e:\n        print(f\"map_parquet_types_to_pandas: {str(e)}\")\n    return type_map    \n\ndef updateBatchControl(df, timestamp_column_name, batch_code, rdb_conn):\n    \"\"\"\n    df: Dataframe containing all the delta records that were written into parquet\n    timestamp_column_name: Column containing the timestamp value (preferably modified date time) in epoch format\n    batch_code: The batch code of the table archival script (PK from batch_control table)\n    rdb_conn: Redshift connection object\n    \"\"\"\n    if df.empty:\n        return True\n\n    try:\n        # Calculate the max(timestamp) from the given timestamp column in the dataframe.\n        max_timestamp = res[timestamp_column_name].max()\n        # Convert epoch number into formatted string representation\n        next_start_time = datetime.fromtimestamp(max_timestamp / 1000.0).strftime('%Y-%m-%d %H:%M:%S.%f')\n        # Prepare update SQL for batch control\n        updsql = \"update stage.batch_control set LAST_TIMESTAMP=%s where BATCH_CODE=%s\"\n        updparams = [next_start_time, batch_code]\n        updresult = rdb_conn.execute(updsql, updparams)\n\n        if updresult is None or updresult != 1:\n            raise Exception(\"Batch_control update failed\")\n    except Exception as e:\n        print(f\"updateBatchControl: {str(e)}\")\n        return False\n    return True","cell_type":"code","execution_count":9},{"metadata":{"trusted":true},"outputs":[],"source":"class RedshiftDb(object):\n    def __init__(self, config):\n        self.env = config.get(\"env\")\n        self.logger = logging.getLogger(\"RedshiftDb\")\n        self.dbConn = self._get_db_connection(config)\n\n    def _get_db_connection(self, config):\n        dbConn = None\n        try:\n            dbConn = redshift_connector.connect(host=config.get(\"REDSHIFT_HOST\"),\n                                                port=config.get(\"REDSHIFT_PORT\"),\n                                                database=config.get(\"REDSHIFT_DATABASE\"),\n                                                user=config.get(\"REDSHIFT_USER\"),\n                                                password=config.get(\"REDSHIFT_PWD\")\n                                                )\n            if not dbConn:\n                raise Exception(\"Could not connect\")\n        except Exception as e:\n            self.logger.error(f\"connect: {str(e)}\")\n        return dbConn\n\n\n    def fetchAll(self, sql, params=None):\n        \"\"\"\n        Takes a SQL string optional list of parameters as input\n        Returns pandas dataframe if the query was successful and has records.\n        On error or when result is empty, returns a None\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            cursor.execute(sql, params)\n            result = cursor.fetch_dataframe()\n\n        except Exception as e:\n            self.logger.error(f\"fetchall: {str(e)}\")\n        return result\n\n\n    def execute(self, sql, params=None):\n        \"\"\"\n        Executes one sql with an optional list of parameters\n        Returns number of records affected, when the SQL is successful. None in case of errors\n        \"\"\"\n        if not params:\n            params = []\n\n        result = None\n        try:\n            if not self.dbConn:\n                raise Exception(\"Redshift connection not available\")\n\n            cursor = self.dbConn.cursor()\n            res = cursor.execute(sql, params)\n            self.dbConn.commit()\n            result = res.rowcount\n\n        except Exception as e:\n            self.logger.error(f\"execute: {str(e)}\")\n            if self.dbConn:\n                self.dbConn.rollback()\n        return result","cell_type":"code","execution_count":10},{"metadata":{},"outputs":[],"source":"Start ->\n\nTest The Connection with bucket","cell_type":"markdown","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"#Connection\nprint(f\"Starting Batch Job: Incremental dump of i2c transaction history live table data into Parquet\")\nconfig = getEnvConfig(env=\"DEV\") # Change env=\"PROD\" while deploying to production\nif not config:\n    print(f\"ERROR: Could not get credential configurations.\")","cell_type":"code","execution_count":11},{"metadata":{"trusted":true},"outputs":[],"source":"s3obj = AwsS3(config)\ns3obj.initialize()\n\n#rdb = RedshiftDb(config)  # <<<<<---- Uncomment once Sprinkle moves to neumoney n/w\nbatch_code = \"CREDIT_CARD_I2C_TXN_HISTORY_LIVE_PARQUET\"\ntimestamp_column = \"modified_date\"\n\nglue_database = \"archive_redshift_db\"\nglue_table = \"credit_card_i2c_transaction_history_live\"\n\nfile_prefix = \"application_data/credit_card/i2c_transaction_history_live_data_v0\"\npartition_cols = [\"part_created_month\", \"part_created_date\"]","cell_type":"code","execution_count":12},{"metadata":{"trusted":true},"outputs":[],"source":"# Read the delta records from credit card tables, using the explore object. (Explore name: credit_card_i2c_transaction_history_live_detail_data)\nexplore_id ='9bd6199d717440f2b10a561368450df4'\ndf = sp.read_explore(explore_id)\n\nsql = (\"SELECT thl.id, \"\n    \" thl.card_reference_id, \"\n    \" thl.message_type_identifier, \"\n    \" thl.notification_event_id, \"\n    \" thl.acquirer_details, \"\n    \" thl.transaction_details,\"\n    \" thl.transaction_date,\"\n    \" thl.transaction_time,\"\n    \" thl.i2c_transaction_type,\"\n    \" thl.transaction_amount,\"\n    \" thl.transaction_posted_date,\"\n    \" thl.available_balance,\"\n    \" thl.original_trans_id,\"\n    \" thl.transaction_status,\"\n    \" thl.transaction_currency,\"\n    \" thl.transaction_description,\"\n    \" thl.service, \"\n    \" thl.mcc, \"\n    \" thl.category_id, \"\n    \" thl.transfer_id, \"\n    \" thl.device_type, \"\n    \" thl.pan_sequence_number, \"\n    \" thl.interchange_fee, \"\n    \" thl.card_present,\"\n    \" thl.created_by, \"\n    \" thl.created_date,\" \n    \" thl.modified_by, \"\n    \" thl.modified_date,\" \n    \" thl.transaction_id,\" \n    \" thl.merchant_name_location, \"\n    \" thl.trans_type,\"\n    \" TIMESTAMP 'epoch' + thl.modified_date/1000 *INTERVAL '1 second' as modified_timestamp, \"\n    \" TIMESTAMP 'epoch' + thl.created_date/1000 *INTERVAL '1 second' as created_timestamp \"\n    \" FROM stage.ds_postgres_credit_card_i2c_transaction_history_live_redshift_i2c_transaction_history_live thl \"\n    \" left join stage.batch_control bc  \"\n    \" on date_add('ms',thl.modified_date,'1970-01-01')  >= bc.last_timestamp \"\n    \" where bc.batch_code = %s;\"\n    )\nsql_params = [batch_code]\n# Uncomment once Sprinkle has moved to neumoney network\n# Fetch data from stage table in redshift\n# df = rdb.fetchAll(sql, sql_params)\n\n\n\n# Convert the application created datetime into YYYYMMDD format for partitioning key\ndf[\"part_created_month\"] = df[\"created_timestamp\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m\"))  \ndf[\"part_created_date\"] = df[\"created_timestamp\"].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m%d\"))  \n\nprint(f\"Fetched {df.shape[0]} records with {df.shape[1]} columns to be added into [{file_prefix}] S3 file\")","cell_type":"code","execution_count":13},{"metadata":{"trusted":true},"outputs":[],"source":"# Columns used to identify unique records from the data set\nmerge_keys = [\"id\",\"card_reference_id\",\"transaction_date\",\"created_date\",\"part_created_month\", \"part_created_date\"]\n# merge_keys_JSON[0] shauld be primary key and merge_keys_JSON[1] shauld be created date\nmerge_keys_JSON = ['transaction_id','created_timestamp','card_reference_id','id','part_created_month','part_created_date']\n\nparquet_definition = {'id':'bigint','card_reference_id':'string','message_type_identifier':'string','notification_event_id':'string',\n                'acquirer_details':'string','transaction_details':'string','transaction_date':'bigint','transaction_time':'bigint',\n                'i2c_transaction_type':'string','transaction_amount':'double','transaction_posted_date':'bigint','available_balance':'double',\n                'original_trans_id':'string','transaction_status':'string','transaction_currency':'string','transaction_description':'string',\n                'service':'string','mcc':'string','category_id':'bigint','transfer_id':'string','device_type':'string','pan_sequence_number':'string',\n                'interchange_fee':'double','card_present':'string','created_by':'bigint','created_date':'bigint','modified_by':'bigint',\n                'modified_date':'bigint','transaction_id':'bigint','merchant_name_location':'string','trans_type':'string','json_data':'string'}\n\ndf_schema = list(parquet_definition.keys())\ndf_schema.extend(partition_cols)\n\n# Any calculated columns, process here\n# Bring these column values from the JSON log storage\n# None\nparquet_schema = list(parquet_definition.keys())\nparquet_schema.extend(partition_cols)\n\n# Convert column data types in the delta dataframe, inorder to match the data types of the existing parquet file\n# If the delta dataframe's data types for merge keys are not matching with parquet file's data types, while merging these two (inorder to remove duplicates), will not find matching records and will cause duplicates\npandas_type_map = map_parquet_types_to_pandas(merge_keys, parquet_definition)\nprint(pandas_type_map)\n\n# Convert pandas dataframe column types using the map\ndf = df.astype(pandas_type_map)","cell_type":"code","execution_count":17},{"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":"json_col_name = 'json_data'\njson_file_path = 'testing-dataset/test1/credit_card'\ntable_name = 'credit_card_i2c_txn_his_live_data'\n\nlst_json = getJsonData(df,merge_keys_JSON,json_col_name,json_file_path,table_name)\njson_df = pd.DataFrame(lst_json)\n","cell_type":"code","execution_count":18},{"metadata":{"trusted":true},"outputs":[],"source":"df1 = pd.merge(df,json_df, how=\"left\", on=merge_keys_JSON)\n\ndf_new = df1[df_schema].copy()\n# [....,json_data, part_created_month, part_created_date]","cell_type":"code","execution_count":19},{"metadata":{"trusted":true},"outputs":[],"source":"# Merge the delta records fetched from the explore object with existing parquet file in S3, so that every affected partition is reconstructed in the dataframe. \n# Note: Overlapping partitions will be overwritten in S3.\n\nfinal_df = mergeDeltaWithExistingParquet(df, file_prefix, partition_cols, merge_keys, df_schema, s3obj)\nprint(final_df.shape)","cell_type":"code","execution_count":17},{"metadata":{"trusted":true},"outputs":[],"source":"# # Write the data into parquet, with the parquet schema.\nresp = write2Parquet(df_new, file_prefix, partition_cols, merge_keys, parquet_schema, s3obj, parquet_definition, glue_database, glue_table)\nprint(resp)","cell_type":"code","execution_count":18},{"metadata":{"trusted":true},"outputs":[],"source":"# Update batch control record for the batch job with the latest time stamp\nif resp.get(\"status\") and resp.get(\"records_written\") > 0:\n    #updateBatchControl(final_df, timestamp_column, batch_code, rdb)\n    print(f\"Updated batch_control: {batch_code}\")\nelse:\n    print(f\"No records written. batch_control not updated\")","cell_type":"code","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{"trusted":true},"outputs":[],"source":"","cell_type":"code","execution_count":0},{"metadata":{},"outputs":[],"source":"","cell_type":"code","execution_count":0}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.10.8","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":2}